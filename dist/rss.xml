<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>Two and One</title><description>learn like a kid.</description><link>https://hjcheng0602.github.io</link><item><title>VGGT读后有感</title><link>https://hjcheng0602.github.io/blog/vggt/vggt</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/vggt/vggt</guid><description>本人读完VGGT:Visual Geometry Grounded Transformer之后的感想喵</description><pubDate>Thu, 14 Aug 2025 20:04:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Spoiler } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;继写完&lt;strong&gt;SLAM3R&lt;/strong&gt;的onlinee处理后，我又将目光投向了今年CVPR的最佳论文：&lt;a href=&quot;https://github.com/facebookresearch/vggt&quot;&gt;VGGT:Visual Geometry Grounded Transformer&lt;/a&gt; 不要问我研究3R为什么不先看vggt😂,问就是我太摆了一开始懒得看了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VGGT&lt;/strong&gt;主要介绍了一个离线的多视图重建，位姿估计和轨迹追踪的强大的模型，与之前类似于&lt;em&gt;SfM&lt;/em&gt;、&lt;strong&gt;DUst3R&lt;/strong&gt;的重建方法相比，它的先进之处在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;摆脱了这些方法所依赖的昂贵的后处理过程（而这通常没有计入到之前模型的性能评估中）&lt;/li&gt;
&lt;li&gt;将多个任务：深度估计、位姿估计、视图重建、轨迹追踪等全部输出，表现甚至超过了之前单一领域的&lt;strong&gt;SOTA&lt;/strong&gt;方法。&lt;/li&gt;
&lt;li&gt;在将多个任务的结果全部输出的过程中，作者发现了引入不同结果之间的内在数学联系限制后会大幅提高模型的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;项目架构&lt;/h2&gt;
&lt;p&gt;与之前的模块化解决问题不同，&lt;strong&gt;VGGT&lt;/strong&gt;的主要结构是一个大的Transformer，它接受一个图片集作为输入，然后输出场景图片的不同三维属性。&lt;/p&gt;
&lt;p&gt;值得一提的是，它所能解决的多视角三维属性几乎涵盖了三维视觉的方方面面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相机位姿以及内参&lt;/li&gt;
&lt;li&gt;点图重建&lt;/li&gt;
&lt;li&gt;关键区域追踪&lt;/li&gt;
&lt;li&gt;关于单张图片的深度图&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;并且，VGGT通过更加创新的举动，它将输出的多任务成果的内在几何关系作为归纳偏置整合进了模型，并发现了大幅度的性能提升，这个很值得去研究。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;感觉&lt;strong&gt;VGGT&lt;/strong&gt;就是一个巨大的transformer，通过极其暴力的手段解决问题，客观上来说，这确实展示了transformer在三维重建领域的应用，但其实我是有一些疑问的：
像自然语言处理这种工作，它是无法定量化去研究的，所以我们引入了transformer，似乎是用未知对抗不确定性的手段，但是，在这个三维重建这个领域，它真的有那么多不确定性吗？
还是感觉transformer对于三维重建的成果属于是结果能看，但是要达到更高的精度会让人很迷惑。&lt;/p&gt;</content:encoded><h:img src="/_astro/image.CZ_2N9gA.png"/><enclosure url="/_astro/image.CZ_2N9gA.png"/></item><item><title>为SLAM3R补充实时处理函数方法</title><link>https://hjcheng0602.github.io/blog/slam3r_online-edit/slam3r_online_contribute</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/slam3r_online-edit/slam3r_online_contribute</guid><description>原本的SLAM3R的recon.py的处理顺序是一个offline的逻辑，将其添加了online处理的recon_online.py</description><pubDate>Tue, 12 Aug 2025 15:57:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Spoiler } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;p&gt;在上个周阅读&lt;strong&gt;SLAM3R&lt;/strong&gt;论文结束后，学长让我去看一下它的&lt;a href=&quot;https://github.com/PKU-VCL-3DV/SLAM3R&quot;&gt;源代码&lt;/a&gt;，读完代码之后，发现虽然论文里讲述的是“可以实时重建”，但是实际上在&lt;code&gt;recon.py&lt;/code&gt;文件中的&lt;code&gt;scene_recon_pipeline&lt;/code&gt;函数中，代码采取了先对所有&lt;code&gt;input_views&lt;/code&gt;进行输入到&lt;code&gt;i2p_model&lt;/code&gt;得到&lt;code&gt;res_feats&lt;/code&gt;，然后再将所有图片的token输入到l2w网络中进行重建的大致逻辑。&lt;/p&gt;
&lt;p&gt;显然，这样的处理方法不是论文里所提出的&lt;strong&gt;online&lt;/strong&gt;处理方法，因此，在过去的一个周里，本人一边练着科三显然今天上午刚挂掉，该死的直线行驶😡，同时抽出了一点点时间完成了&lt;code&gt;recon_online.py&lt;/code&gt;,一个把原本的&lt;code&gt;scene_recon_pipeline&lt;/code&gt;改成&lt;code&gt;online&lt;/code&gt;处理的改动。&lt;/p&gt;
&lt;h2&gt;原函数的处理逻辑&lt;/h2&gt;
&lt;p&gt;阅读原函数的代码，我们可以将其分为以下几段：&lt;/p&gt;
&lt;h3&gt;预处理&amp;#x26;得到所有view的token&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Pre-save the RGB images along with their corresponding masks 
# in preparation for visualization at last.
rgb_imgs = []
for i in range(len(data_views)):
    if data_views[i][&apos;img&apos;].shape[0] == 1:
        data_views[i][&apos;img&apos;] = data_views[i][&apos;img&apos;][0]        
    rgb_imgs.append(transform_img(dict(img=data_views[i][&apos;img&apos;][None]))[...,::-1])
if &apos;valid_mask&apos; not in data_views[0]:
    valid_masks = None
else:
    valid_masks = [view[&apos;valid_mask&apos;] for view in data_views]   

#preprocess data for extracting their img tokens with encoder
for view in data_views:
    view[&apos;img&apos;] = torch.tensor(view[&apos;img&apos;][None])
    view[&apos;true_shape&apos;] = torch.tensor(view[&apos;true_shape&apos;][None])
    for key in [&apos;valid_mask&apos;, &apos;pts3d_cam&apos;, &apos;pts3d&apos;]:
        if key in view:
            del view[key]
    to_device(view, device=args.device)
# pre-extract img tokens by encoder, which can be reused 
# in the following inference by both i2p and l2w models
res_shapes, res_feats, res_poses = get_img_tokens(data_views, i2p_model)    # 300+fps
print(&apos;finish pre-extracting img tokens&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里重点就是最后的&lt;code&gt;res_shapes, res_feats, res_poses = get_img_tokens(data_views, i2p_model)&lt;/code&gt;，采用&lt;code&gt;i2p_model&lt;/code&gt;的&lt;code&gt;_encode_multiview&lt;/code&gt;方法批次化地(&lt;em&gt;batchify&lt;/em&gt;)对&lt;code&gt;data_views&lt;/code&gt;进行处理，从而得到所有的view的&lt;code&gt;token&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;对所有view进行推理得到最合适的key_frame_stride&lt;/h3&gt;
&lt;p&gt;这里的核心代码就是：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# decide the stride of sampling keyframes, as well as other related parameters
if args.keyframe_stride == -1:
    kf_stride = adapt_keyframe_stride(input_views, i2p_model, 
                                        win_r = 3,
                                        adapt_min=args.keyframe_adapt_min,
                                        adapt_max=args.keyframe_adapt_max,
                                        adapt_stride=args.keyframe_adapt_stride)
else:
    kf_stride = args.keyframe_stride
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;code&gt;adapt_keyframe_stride&lt;/code&gt;函数是一个典型的&lt;strong&gt;offline&lt;/strong&gt;处理函数，它的功能是在所有的input_view中遍历可能的&lt;code&gt;kf_stride&lt;/code&gt;取值，然后对每一个可能的取值随机取样，然后利用&lt;code&gt;i2p_inference_batch&lt;/code&gt;函数得出置信度作为相似度？然后选取最高的所对应的&lt;code&gt;kf_stride&lt;/code&gt;作为最优的取值。&lt;/p&gt;
&lt;h3&gt;使用初始的几个滑动窗口创建初始的全局scene&amp;#x26;初始化buffer set&lt;/h3&gt;
&lt;p&gt;因为&lt;strong&gt;SLAM3R&lt;/strong&gt;初始化时的&lt;a href=&quot;http://localhost:4321/blog/slam3r/slam3r&quot;&gt;特殊性&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于第一个帧这种特殊情况，我们采用了重复运行多次I2P获取足够多数量的初始帧作为缓冲集&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在原本的offline格式的&lt;code&gt;recon.py&lt;/code&gt;中，这种做法以这种样式呈现：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride], 
                                                i2p_model, 
                                                winsize=initial_winsize,
                                                return_ref_id=True) # 5*(1,224,224,3)

# start reconstrution of the whole scene
init_num = len(initial_pcds)
per_frame_res = dict(i2p_pcds=[], i2p_confs=[], l2w_pcds=[], l2w_confs=[])
for key in per_frame_res:
    per_frame_res[key] = [None for _ in range(num_views)]

registered_confs_mean = [_ for _ in range(num_views)]

# set up the world coordinates with the initial window
for i in range(init_num):
    per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] = initial_confs[i][0].to(args.device)  # 224,224
    registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()

# initialize the buffering set with the initial window
assert args.buffer_size &amp;#x3C;= 0 or args.buffer_size &gt;= init_num 
buffering_set_ids = [i*kf_stride for i in range(init_num)]

# set up the world coordinates with frames in the initial window
for i in range(init_num):
    input_views[i*kf_stride][&apos;pts3d_world&apos;] = initial_pcds[i]
    
initial_valid_masks = [conf &gt; conf_thres_i2p for conf in initial_confs] # 1,224,224
normed_pts = normalize_views([view[&apos;pts3d_world&apos;] for view in input_views[:init_num*kf_stride:kf_stride]],
                                            initial_valid_masks)
for i in range(init_num):
    input_views[i*kf_stride][&apos;pts3d_world&apos;] = normed_pts[i]
    # filter out points with low confidence
    input_views[i*kf_stride][&apos;pts3d_world&apos;][~initial_valid_masks[i]] = 0       
    per_frame_res[&apos;l2w_pcds&apos;][i*kf_stride] = normed_pts[i]  # 224,224,3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride], 
                                                   i2p_model, 
                                                   winsize=initial_winsize,
                                                   return_ref_id=True) # 5*(1,224,224,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这一行是对初始化的几个&lt;code&gt;view_token&lt;/code&gt;进行场景重建，并选出一开始的&lt;code&gt;init_ref_id&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然后之后就是把所有初始化的帧放到&lt;code&gt;buffer_set&lt;/code&gt;里，然后进行一些归一化处理。&lt;/p&gt;
&lt;h3&gt;对原始的view再继续进行i2p重建点图&lt;/h3&gt;
&lt;p&gt;这里我们重新遍历所有图像，对应论文里面通过&lt;code&gt;I2P&lt;/code&gt;的&lt;code&gt;decoder&lt;/code&gt;重建所有&lt;code&gt;view&lt;/code&gt;的点图。此外，注意&lt;code&gt;initial window&lt;/code&gt;的关键帧图片基本上已经在上面的初始化中被创建出了点图，因此我们选择略过他们，只对没有被创建点图的帧进行&lt;code&gt;I2P&lt;/code&gt;处理
以得到点图，然后就采用论文中的输入窗口多个帧，重建每个帧的点云作为&lt;code&gt;L2W model&lt;/code&gt;的输入。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for view_id in tqdm(range(num_views), desc=&quot;I2P resonstruction&quot;):
    # skip the views in the initial window
    if view_id in buffering_set_ids:
        # trick to mark the keyframe in the initial window
        if view_id // kf_stride == init_ref_id:
            per_frame_res[&apos;i2p_pcds&apos;][view_id] = per_frame_res[&apos;l2w_pcds&apos;][view_id].cpu()
        else:
            per_frame_res[&apos;i2p_pcds&apos;][view_id] = torch.zeros_like(per_frame_res[&apos;l2w_pcds&apos;][view_id], device=&quot;cpu&quot;)
        per_frame_res[&apos;i2p_confs&apos;][view_id] = per_frame_res[&apos;l2w_confs&apos;][view_id].cpu()
        continue
    # construct the local window 
    sel_ids = [view_id]
    for i in range(1,win_r+1):
        if view_id-i*adj_distance &gt;= 0:
            sel_ids.append(view_id-i*adj_distance)
        if view_id+i*adj_distance &amp;#x3C; num_views:
            sel_ids.append(view_id+i*adj_distance)
    local_views = [input_views[id] for id in sel_ids]
    ref_id = 0 
    # recover points in the local window, and save the keyframe points and confs
    output = i2p_inference_batch([local_views], i2p_model, ref_id=ref_id, 
                                tocpu=False, unsqueeze=False)[&apos;preds&apos;]
    #save results of the i2p model
    per_frame_res[&apos;i2p_pcds&apos;][view_id] = output[ref_id][&apos;pts3d&apos;].cpu() # 1,224,224,3
    per_frame_res[&apos;i2p_confs&apos;][view_id] = output[ref_id][&apos;conf&apos;][0].cpu() # 224,224

    # construct the input for L2W model        
    input_views[view_id][&apos;pts3d_cam&apos;] = output[ref_id][&apos;pts3d&apos;] # 1,224,224,3
    valid_mask = output[ref_id][&apos;conf&apos;] &gt; conf_thres_i2p # 1,224,224
    input_views[view_id][&apos;pts3d_cam&apos;] = normalize_views([input_views[view_id][&apos;pts3d_cam&apos;]],
                                                [valid_mask])[0]
    input_views[view_id][&apos;pts3d_cam&apos;][~valid_mask] = 0 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;对初始窗口非关键帧进行注册&lt;/h3&gt;
&lt;p&gt;显然我们在之前的初始化场景中只注册了关键帧，因此我们现在开始对非关键帧进行注册：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Special treatment: register the frames within the range of initial window with L2W model
# TODO: batchify
if kf_stride &gt; 1:
    max_conf_mean = -1
    for view_id in tqdm(range((init_num-1)*kf_stride), desc=&quot;pre-registering&quot;):  
        if view_id % kf_stride == 0:
            continue
        # construct the input for L2W model
        l2w_input_views = [input_views[view_id]] + [input_views[id] for id in buffering_set_ids]
        # (for defination of ref_ids, see the doc of l2w_model)
        output = l2w_inference(l2w_input_views, l2w_model, 
                                ref_ids=list(range(1,len(l2w_input_views))), 
                                device=args.device,
                                normalize=args.norm_input)
        
        # process the output of L2W model
        input_views[view_id][&apos;pts3d_world&apos;] = output[0][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
        conf_map = output[0][&apos;conf&apos;] # 1,224,224
        per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0] # 224,224
        registered_confs_mean[view_id] = conf_map.mean().cpu()
        per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
        
        if registered_confs_mean[view_id] &gt; max_conf_mean:
            max_conf_mean = registered_confs_mean[view_id]
    print(f&apos;finish aligning {(init_num-1)*kf_stride} head frames, with a max mean confidence of {max_conf_mean:.2f}&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里正如注释所说，是一个&lt;strong&gt;Special treatment&lt;/strong&gt;。也是一个特殊情况处理。&lt;/p&gt;
&lt;h4&gt;缩放confs&lt;/h4&gt;
&lt;p&gt;我们发现，我们只用&lt;code&gt;l2w&lt;/code&gt;网络对非关键帧进行了置信度预测，关键帧的置信度是由之前的&lt;code&gt;i2p&lt;/code&gt;网络进行预测的，作者在这里为了控制计算成本，选择直接将后者乘上一个常数因子进行缩放，大致反映出了场景的置信度分数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# A problem is that the registered_confs_mean of the initial window is generated by I2P model,
# while the registered_confs_mean of the frames within the initial window is generated by L2W model,
# so there exists a gap. Here we try to align it.
max_initial_conf_mean = -1
for i in range(init_num):
    if registered_confs_mean[i*kf_stride] &gt; max_initial_conf_mean:
        max_initial_conf_mean = registered_confs_mean[i*kf_stride]
factor = max_conf_mean/max_initial_conf_mean
# print(f&apos;align register confidence with a factor {factor}&apos;)
for i in range(init_num):
    per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] *= factor
    registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;对剩下的views进行注册&lt;/h3&gt;
&lt;p&gt;OK，经过了以上的对于初始帧的特殊处理，我们终于踏入了正途：在过程中对每个帧进行实时处理&lt;/p&gt;
&lt;h4&gt;从buffer set里选择最相近的sel_num个帧：&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# select sccene frames in the buffering set to work as a global reference
cand_ref_ids = buffering_set_ids
ref_views, sel_pool_ids = scene_frame_retrieve(
    [input_views[i] for i in cand_ref_ids], 
    input_views[ni:ni+num_register:2], 
    i2p_model, sel_num=num_scene_frame, 
    # cand_recon_confs=[per_frame_res[&apos;l2w_confs&apos;][i] for i in cand_ref_ids],
    depth=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里正如论文中所述，采用了&lt;code&gt;i2p_model&lt;/code&gt;的前2个&lt;strong&gt;decoder&lt;/strong&gt;进行相似评分。&lt;/p&gt;
&lt;h4&gt;将选取的最相近的几个帧作为参考合并当前帧进行l2w重建&lt;/h4&gt;
&lt;p&gt;显而易见，言以概之：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# register the source frames in the local coordinates to the world coordinates with L2W model
l2w_input_views = ref_views + input_views[ni:max_id+1]
input_view_num = len(ref_views) + max_id - ni + 1
assert input_view_num == len(l2w_input_views)

output = l2w_inference(l2w_input_views, l2w_model, 
                        ref_ids=list(range(len(ref_views))), 
                        device=args.device,
                        normalize=args.norm_input)

# process the output of L2W model
src_ids_local = [id+len(ref_views) for id in range(max_id-ni+1)]  # the ids of src views in the local window
src_ids_global = [id for id in range(ni, max_id+1)]    #the ids of src views in the whole dataset
succ_num = 0
for id in range(len(src_ids_global)):
    output_id = src_ids_local[id] # the id of the output in the output list
    view_id = src_ids_global[id]    # the id of the view in all views
    conf_map = output[output_id][&apos;conf&apos;] # 1,224,224
    input_views[view_id][&apos;pts3d_world&apos;] = output[output_id][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
    per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0]
    registered_confs_mean[view_id] = conf_map[0].mean().cpu()
    per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
    succ_num += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;import { Aside } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;h4&gt;通过一些手段更新buffer set&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;buffer_set&lt;/code&gt;的选取方法差不多就和论文里面讲的一样，基本上就是随机选取了。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# update the buffering set
if next_register_id - milestone &gt;= update_buffer_intv:  
    while(next_register_id - milestone &gt;= kf_stride):
        candi_frame_id += 1
        full_flag = max_buffer_size &gt; 0 and len(buffering_set_ids) &gt;= max_buffer_size
        insert_flag = (not full_flag) or ((strategy == &apos;fifo&apos;) or 
                                            (strategy == &apos;reservoir&apos; and np.random.rand() &amp;#x3C; max_buffer_size/candi_frame_id))
        if not insert_flag: 
            milestone += kf_stride
            continue
        # Use offest to ensure the selected view is not too close to the last selected view
        # If the last selected view is 0, 
        # the next selected view should be at least kf_stride*3//4 frames away
        start_ids_offset = max(0, buffering_set_ids[-1]+kf_stride*3//4 - milestone)
            
        # get the mean confidence of the candidate views
        mean_cand_recon_confs = torch.stack([registered_confs_mean[i]
                                    for i in range(milestone+start_ids_offset, milestone+kf_stride)])
        mean_cand_local_confs = torch.stack([local_confs_mean[i]
                                    for i in range(milestone+start_ids_offset, milestone+kf_stride)])
        # normalize the confidence to [0,1], to avoid overconfidence
        mean_cand_recon_confs = (mean_cand_recon_confs - 1)/mean_cand_recon_confs # transform to sigmoid
        mean_cand_local_confs = (mean_cand_local_confs - 1)/mean_cand_local_confs
        # the final confidence is the product of the two kinds of confidences
        mean_cand_confs = mean_cand_recon_confs*mean_cand_local_confs
        
        most_conf_id = mean_cand_confs.argmax().item()
        most_conf_id += start_ids_offset
        id_to_buffer = milestone + most_conf_id
        buffering_set_ids.append(id_to_buffer)
        # print(f&quot;add ref view {id_to_buffer}&quot;)                
        # since we have inserted a new frame, overflow must happen when full_flag is True
        if full_flag:
            if strategy == &apos;reservoir&apos;:
                buffering_set_ids.pop(np.random.randint(max_buffer_size))
            elif strategy == &apos;fifo&apos;:
                buffering_set_ids.pop(0)
        # print(next_register_id, buffering_set_ids)
        milestone += kf_stride
# transfer the data to cpu if it is not in the buffering set, to save gpu memory
for i in range(next_register_id):
    to_device(input_views[i], device=args.device if i in buffering_set_ids else &apos;cpu&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;保存环节&lt;/h3&gt;
&lt;p&gt;当我们处理完所有帧后，我们会保存我们的所有帧的点云，把这些所有帧的点云合到一起进行重建，得出最后的场景点云。&lt;/p&gt;
&lt;h3&gt;review&lt;/h3&gt;
&lt;p&gt;显而易见，原&lt;code&gt;recon.py&lt;/code&gt;中的这个&lt;code&gt;pipeline&lt;/code&gt;是一个完全的&lt;strong&gt;offline&lt;/strong&gt;处理方法，因此，我编写了一个真正的（？&lt;strong&gt;online&lt;/strong&gt;版本的方法，处理逻辑如下所示：&lt;/p&gt;
&lt;h2&gt;online 函数的处理逻辑&lt;/h2&gt;
&lt;p&gt;既然是要online，我们显然第一件要做的事情就是写下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(len(data_views)):
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后我们在进行一系列处理：&lt;/p&gt;
&lt;h3&gt;预处理 &amp;#x26; 得到当前view的token&lt;/h3&gt;
&lt;p&gt;显然，通过对原先&lt;strong&gt;offline&lt;/strong&gt;版本的函数分析，这个过程没有初始化的困扰，因此，我们可以大胆对所有遍历到的view都进行这一步：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Pre-save the RGB images along with their corresponding masks
# in preparation for visualization at last.

if data_views[i][&apos;img&apos;].shape[0] == 1:
    data_views[i][&apos;img&apos;] = data_views[i][&apos;img&apos;][0]
rgb_imgs.append(transform_img(dict(img=data_views[i][&apos;img&apos;][None]))[...,::-1])

if is_have_mask_rgb:
    valid_masks.append(data_views[i][&apos;valid_mask&apos;])

# process now image for extracting its img token with encoder
data_views[i][&apos;img&apos;] = torch.tensor(data_views[i][&apos;img&apos;][None])
data_views[i][&apos;true_shape&apos;] = torch.tensor(data_views[i][&apos;true_shape&apos;][None])
for key in [&apos;valid_mask&apos;, &apos;pts3d_cam&apos;, &apos;pts3d&apos;]:
    if key in data_views[i]:
        del data_views[key]
to_device(data_views[i], device=args.device)

# pre-extract img tokens by encoder, which can be reused 
# in the following inference by both i2p and l2w models
temp_shape, temp_feat, temp_pose = get_single_img_tokens([data_views[i]], i2p_model, True)
res_shapes.append(temp_shape[0])
res_feats.append(temp_feat[0])
res_poses.append(temp_pose[0])
print(f&quot;finish pre-extracting img token of view {i}&quot;)

input_views.append(dict(label=data_views[i][&apos;label&apos;],
                        img_tokens=temp_feat[0],
                        true_shape=data_views[i][&apos;true_shape&apos;],
                        img_pos=temp_pose[0]))
for key in per_frame_res:
    per_frame_res[key].append(None)
registered_confs_mean.append(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里我使用了一个&lt;code&gt;get_single_img_tokens&lt;/code&gt;函数，与之前的&lt;code&gt;get_img_tokens&lt;/code&gt;函数相比，该函数除了不能batch化(online的限制)之外，效果输出别无二致。&lt;/p&gt;
&lt;h3&gt;积累帧以用于场景初始化&lt;/h3&gt;
&lt;p&gt;需要注意的是，当帧序数小于初始化所需要的帧数时，我们后续的程序均无法进行，因此在我的代码中，我选择直接跳过，先蓄势待发🤣&lt;/p&gt;
&lt;p&gt;一旦积累到初始化场景所需帧后，函数会采用一系列操作初始化场景以及初始化buffer set，对初始化后的各帧点云进行归一化处理：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# accumulate the initial window frames
if i &amp;#x3C; (initial_winsize - 1)*kf_stride and i % kf_stride == 0:
    continue
elif i == (initial_winsize - 1)*kf_stride:
    initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride],
                                                                i2p_model,
                                                                winsize=initial_winsize,
                                                                return_ref_id=True)
    # set up the world coordinates with the initial window
    init_num = len(initial_pcds)
    for j in range(init_num):
        per_frame_res[&apos;l2w_confs&apos;][j * kf_stride] = initial_confs[j][0].to(args.device)
        registered_confs_mean[j * kf_stride] = per_frame_res[&apos;l2w_confs&apos;][j * kf_stride].mean().cpu()
    # initialize the buffering set with the initial window
    assert args.buffer_size &amp;#x3C;= 0 or args.buffer_size &gt;= init_num 
    buffering_set_ids = [j*kf_stride for j in range(init_num)]
    # set ip the woeld coordinates with frames in the initial window
    for j in range(init_num):
        input_views[j*kf_stride][&apos;pts3d_world&apos;] = initial_pcds[j]
    initial_valid_masks = [conf &gt; conf_thres_i2p for conf in initial_confs]
    normed_pts = normalize_views([view[&apos;pts3d_world&apos;] for view in input_views[:init_num*kf_stride:kf_stride]],
                                                initial_valid_masks)
    for j in range(init_num):
        input_views[j*kf_stride][&apos;pts3d_world&apos;] = normed_pts[j]
        # filter out points with low confidence
        input_views[j*kf_stride][&apos;pts3d_world&apos;][~initial_valid_masks[j]] = 0
        per_frame_res[&apos;l2w_pcds&apos;][j*kf_stride] = normed_pts[j]

elif i &amp;#x3C; (initial_winsize - 1) * kf_stride:
    continue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要注意的是，这里一旦积累到足够多的初始帧，我们就不会进行continue处理了，然后直接进行下一部分。&lt;/p&gt;
&lt;h3&gt;对之前积累的view进行i2p重建点图（包含正在处理的帧） &amp;#x26; 注册初始窗口非关键帧&lt;/h3&gt;
&lt;p&gt;这里我们采用类似于之前&lt;strong&gt;offline&lt;/strong&gt;的顺序，只不过把外在的表现形式作出了改变，实际上内在的顺序逻辑基本不变：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# first recover the accumulate views
if i == (initial_winsize - 1) * kf_stride:
    for view_id in range(i + 1):
        # skip the views in the initial window
        if view_id in buffering_set_ids:
            # trick to mark the keyframe in the initial window
            if view_id // kf_stride == init_ref_id:
                per_frame_res[&apos;i2p_pcds&apos;][view_id] = per_frame_res[&apos;l2w_pcds&apos;][view_id].cpu()
            else:
                per_frame_res[&apos;i2p_pcds&apos;][view_id] = torch.zeros_like(per_frame_res[&apos;l2w_pcds&apos;][view_id], device=&quot;cpu&quot;)
            per_frame_res[&apos;i2p_confs&apos;][view_id] = per_frame_res[&apos;l2w_confs&apos;][view_id].cpu()
            print(f&quot;finish revocer pcd of frame {view_id} in their local coordinates(in buffer set), with a mean confidence of {per_frame_res[&apos;i2p_confs&apos;][view_id].mean():.2f} up to now.&quot;)
            continue
        # construct the local window with the initial views
        sel_ids = [view_id]
        for j in range(1, win_r + 1):
            if view_id - j * adj_distance &gt;= 0:
                sel_ids.append(view_id - j * adj_distance)
            if view_id + j * adj_distance &amp;#x3C; i:
                sel_ids.append(view_id + j * adj_distance)
        local_views = [input_views[id] for id in sel_ids]
        ref_id = 0

        # recover poionts in the initial window, and save the keyframe points and confs
        output = i2p_inference_batch([local_views], i2p_model, ref_id=ref_id,
                                        tocpu=False, unsqueeze=False)[&apos;preds&apos;]
        # save results of the i2p model for the initial window
        per_frame_res[&apos;i2p_pcds&apos;][view_id] = output[ref_id][&apos;pts3d&apos;].cpu()
        per_frame_res[&apos;i2p_confs&apos;][view_id] = output[ref_id][&apos;conf&apos;][0].cpu()

        # construct the input for L2W model
        input_views[view_id][&apos;pts3d_cam&apos;] = output[ref_id][&apos;pts3d&apos;]
        valid_mask = output[ref_id][&apos;conf&apos;] &gt; conf_thres_i2p
        input_views[view_id][&apos;pts3d_cam&apos;] = normalize_views([input_views[view_id][&apos;pts3d_cam&apos;]],
                                                                [valid_mask])[0]
        input_views[view_id][&apos;pts3d_cam&apos;][~valid_mask] = 0

        local_confs_mean_up2now = [conf.mean() for conf in per_frame_res[&apos;i2p_confs&apos;] if conf is not None]
        print(f&quot;finish revocer pcd of frame {view_id} in their local coordinates, with a mean confidence of {torch.stack(local_confs_mean_up2now).mean():.2f} up to now.&quot;)

    # Special treatment: register the frames within the range of initial window with L2W model
    if kf_stride &gt; 1:
        max_conf_mean = -1
        for view_id in tqdm(range((init_num - 1) * kf_stride), desc=&quot;pre-registering&quot;):
            if view_id % kf_stride == 0:
                continue
            # construct the input for L2W model

            l2w_input_views = [input_views[view_id]] + [input_views[id] for id in buffering_set_ids]
            # (for defination of ref_ids, seee the doc of l2w_model)
            output = l2w_inference(l2w_input_views, l2w_model,
                                    ref_ids=list(range(1,len(l2w_input_views))),
                                    device=args.device,
                                    normalize=args.norm_input)
            # process the output of L2W model
            input_views[view_id][&apos;pts3d_world&apos;] = output[0][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
            conf_map = output[0][&apos;conf&apos;] # 1,224,224
            per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0] # 224,224
            registered_confs_mean[view_id] = conf_map.mean().cpu()
            per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
            
            if registered_confs_mean[view_id] &gt; max_conf_mean:
                max_conf_mean = registered_confs_mean[view_id]
        print(f&apos;finish aligning {(init_num)*kf_stride} head frames, with a max mean confidence of {max_conf_mean:.2f}&apos;)
        # A problem is that the registered_confs_mean of the initial window is generated by I2P model,
        # while the registered_confs_mean of the frames within the initial window is generated by L2W model,
        # so there exists a gap. Here we try to align it.
        max_initial_conf_mean = -1
        for i in range(init_num):
            if registered_confs_mean[i*kf_stride] &gt; max_initial_conf_mean:
                max_initial_conf_mean = registered_confs_mean[i*kf_stride]
        factor = max_conf_mean/max_initial_conf_mean
        # print(f&apos;align register confidence with a factor {factor}&apos;)
        for i in range(init_num):
            per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] *= factor
            registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()
    # register the rest frames with L2W model
    next_register_id = (init_num - 1) * kf_stride + 1
    milestone = init_num * kf_stride + 1
    update_buffer_intv = kf_stride*args.update_buffer_intv   # update the buffering set every update_buffer_intv frames
    max_buffer_size = args.buffer_size
    strategy = args.buffer_strategy
    candi_frame_id = len(buffering_set_ids) # used for the reservoir sampling strategy
    continue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后在处理完这么一堆之后我们直接&lt;code&gt;continue&lt;/code&gt;到下一个循环。&lt;/p&gt;
&lt;h3&gt;处理新图片&lt;/h3&gt;
&lt;p&gt;在下一个循环中，我们拿到了新图片，此时我们也在我们的&lt;strong&gt;online&lt;/strong&gt;函数中踏上了正途，可以对每一个帧进行实时处理了。&lt;/p&gt;
&lt;p&gt;这里，我们的处理逻辑与第一种方法类似，不同的一点是我是一帧一帧地去处理。&lt;/p&gt;
&lt;h3&gt;保存环节&lt;/h3&gt;
&lt;p&gt;与上一个方法略微不同，我提供了参数选项选择是否在线保存/逐几帧保存，因此我重写了一个增量式保存的类：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class IncrementalReconstructor:
    &quot;&quot;&quot;
    A class used for reconstruting the pts incrementally
    &quot;&quot;&quot;
    def __init__(self):
        self.res_pcds = None
        self.res_rgbs = None
        self.res_confs = None
        self.res_valid_masks = None
        self.is_initialized = False

    def add_frame(self, view: dict, img: np.ndarray, conf: np.ndarray = None, valid_mask: np.ndarray = None):
        &quot;&quot;&quot;
        Incrementally add a new frame of view data.

        Args:
            view (dict): a dictionary for a new view
            img (np.ndarray): rgb_img
            conf (np.ndarray, optional): 
            valid_mask (np.ndarray, optional): 
        &quot;&quot;&quot;
        try:
            new_pcd = to_numpy(view[&apos;pts3d_world&apos;]).reshape(-1, 3)
            new_rgb = to_numpy(img).reshape(-1, 3)
        except KeyError:
            print(f&quot;Warning: &apos;pts3d_world&apos; not found in the new view. Frame skipped.&quot;)
            return
        if not self.is_initialized:
            self.res_pcds = new_pcd
            self.res_rgbs = new_rgb
            if conf is not None:
                self.res_confs = to_numpy(conf).reshape(-1)
            if valid_mask is not None:
                self.res_valid_masks = to_numpy(valid_mask).reshape(-1)
            self.is_initialized = True
        else:
            self.res_pcds = np.concatenate([self.res_pcds, new_pcd], axis=0)
            self.res_rgbs = np.concatenate([self.res_rgbs, new_rgb], axis=0)
            if conf is not None:
                new_conf = to_numpy(conf).reshape(-1)
                self.res_confs = np.concatenate([self.res_confs, new_conf], axis=0)
            if valid_mask is not None:
                new_mask = to_numpy(valid_mask).reshape(-1)
                self.res_valid_masks = np.concatenate([self.res_valid_masks, new_mask], axis=0)

    def save_snapshot(self, snapshot_id: int, save_dir: str, num_points_save: int = 200000, conf_thres_res: float = 3.0):
        &quot;&quot;&quot;
        Just save
        &quot;&quot;&quot;
        if not self.is_initialized:
            print(&quot;Warning: Reconstructor not initialized. Nothing to save.&quot;)
            return
        save_name = f&quot;recon_snapshot_{snapshot_id:05d}.ply&quot;
        pts_count = len(self.res_pcds)
        final_valid_mask = np.ones(pts_count, dtype=bool)

        if self.res_valid_masks is not None:
            final_valid_mask &amp;#x26;= self.res_valid_masks
        
        if self.res_confs is not None:
            conf_masks = self.res_confs &gt; conf_thres_res
            final_valid_mask &amp;#x26;= conf_masks

        valid_ids = np.where(final_valid_mask)[0]
        
        if len(valid_ids) == 0:
            print(f&quot;Warning for snapshot {snapshot_id}: No valid points left after filtering.&quot;)
            return
            
        print(f&apos;Snapshot {snapshot_id}: Ratio of points filtered out: {(1. - len(valid_ids) / pts_count) * 100:.2f}%&apos;)
        n_samples = min(num_points_save, len(valid_ids))
        print(f&quot;Snapshot {snapshot_id}: Resampling {n_samples} points from {len(valid_ids)} valid points.&quot;)
        sampled_idx = np.random.choice(valid_ids, n_samples, replace=False)
        sampled_pts = self.res_pcds[sampled_idx]
        sampled_rgbs = self.res_rgbs[sampled_idx]
        save_path = join(save_dir, save_name)
        print(f&quot;Saving reconstruction snapshot to {save_path}&quot;)
        save_ply(points=sampled_pts, save_path=save_path, colors=sampled_rgbs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在每一个循环最后加以调用：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;reconstructor.add_frame(
            view=input_views[i],
            img=rgb_imgs[i],
            conf=per_frame_res[&apos;l2w_confs&apos;][i],
            valid_mask=valid_masks
        )
        if args.save_online:
            if (i + 1) % args.save_frequency == 0:
                reconstructor.save_snapshot(
                    snapshot_id=i + 1,
                    save_dir=save_dir,
                    num_points_save=num_points_save,
                    conf_thres_res=conf_thres_l2w
                )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK，到此为止我就写完了原本的处理逻辑的解释和新写的**onlinee*处理逻辑介绍，其实要说不说，&lt;strong&gt;online&lt;/strong&gt;处理逻辑也并非太过复杂，但是奈何我这几天因为学车耽误了太多时间也没做什么东西（x&lt;/p&gt;
&lt;p&gt;又水了一篇blog😋&lt;/p&gt;
&lt;h2&gt;新的仓库：&lt;/h2&gt;
&lt;p&gt;import { GithubCard } from &apos;astro-pure/advanced&apos;&lt;/p&gt;</content:encoded><h:img src="/_astro/image.CHGvQ2OJ.png"/><enclosure url="/_astro/image.CHGvQ2OJ.png"/></item><item><title>SLAM3R读后有感</title><link>https://hjcheng0602.github.io/blog/slam3r/slam3r</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/slam3r/slam3r</guid><description>本人读完SLAM3R后的理解喵</description><pubDate>Sun, 03 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;最近几天读完了&lt;a href=&quot;https://github.com/PKU-VCL-3DV/SLAM3R&quot;&gt;SLAM3R&lt;/a&gt;的论文，这是2025年CVPR的一 篇&lt;strong&gt;Highlight&lt;/strong&gt;论文，也是我在3R方向的读过的第3篇论文。&lt;/p&gt;
&lt;p&gt;这篇论文主要介绍了一个叫做&lt;strong&gt;SLAM3R&lt;/strong&gt;的根据视频即时重建的系统，感觉是由&lt;strong&gt;DUst3R&lt;/strong&gt;中获得的灵感，不同的是&lt;strong&gt;DUst3R&lt;/strong&gt;是根据两张图片重建出三维点图，并且是离线处理；而&lt;strong&gt;SLAM3R&lt;/strong&gt;是从一个单目视频中实时在线重建，并且相较于之前的一些方法具有极高的效率。&lt;/p&gt;
&lt;h2&gt;SLAM3R的主要模块&lt;/h2&gt;
&lt;p&gt;SLAM3R主要由&lt;strong&gt;I2P&lt;/strong&gt;和&lt;strong&gt;L2W&lt;/strong&gt;两大模块组成，分别负责从视频中的关键帧重建点图(Image to Point)和利用点图增量式地重建全局点图（Local to World）,具体结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./overalmodule.png&quot; alt=&quot;nothing&quot;&gt;&lt;/p&gt;
&lt;h3&gt;视频预处理&lt;/h3&gt;
&lt;p&gt;首先，SLAM3R采用了滑动窗口算法将视频拆成多个小片段，把多个小片段输入到I2P中进行处理。&lt;/p&gt;
&lt;h3&gt;I2P网络&lt;/h3&gt;
&lt;p&gt;I2P模块接受预处理产生的视频片段，该视频片段由多个帧${F_i},i = 1, ... N$组成。通常我们从中选取最中间的帧作为关键帧$F_{key}$，剩下的$N - 1$个帧作为补充帧输入到I2P中。&lt;/p&gt;
&lt;p&gt;首先，我们将所有帧通过一个由$m$个ViT encoder组成的$E_{img}$，生成相应的token，然后再进行decoder操作。具体就是将关键帧的token输入到一个特殊处理的decoder:$D_{key}$里（如下图所示），然后剩下的$N - 1$个补充帧共享同一个decoder结构（继承自&lt;strong&gt;DUst3R&lt;/strong&gt;，由$n$个ViT decoder组成），均生成对应的$G_{sup_i}$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./D_key.png&quot; alt=&quot;$D_key$&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后，我们再使用类似于&lt;strong&gt;DUSt3R&lt;/strong&gt;中的方法，将这些帧（尤其是关键帧）做出一个置信度最高的三维重建。从而得到某一个视频片段对应的点图$\hat{X}_{key}$。&lt;/p&gt;
&lt;h3&gt;L2W网络&lt;/h3&gt;
&lt;p&gt;这个模块接受I2P模块产生的$X_{key}$作为输入，因为其是一个在线处理方法，所以我们引入了缓冲集这一关键的组分。&lt;/p&gt;
&lt;p&gt;首先，我们在已经处理完的关键帧点图中采用&lt;code&gt;reservoir strategy&lt;/code&gt;选取$B$个已经注册完的帧作为缓冲集（对于第一个帧这种特殊情况，我们采用了重复运行多次I2P获取足够多数量的初始帧作为缓冲集），然后，每当一个新的帧输入时，我们使用一个检索模块（由I2P中的decoder组成）在缓冲集中将特征的相似度进行匹配，我们然后选取匹配度最高的$K$个关键帧点图，然后将这$K$个关键帧点图 $\hat{X}_{i}^{H \times W \times 3},i = 1 , ..., K + 1$作为这个模块的输入。&lt;/p&gt;
&lt;p&gt;如前图所示，我们将这$K + 1$个点图输入到我们的L2W模块的encoder $E_{pts}$ 中：
$$
\mathcal{P}&lt;em&gt;i^{(T\times d)}=E&lt;/em&gt;{pts}(\hat{X}_i^{(H\times W\times3)}),i=1,...,K+1.
$$
然后，由于我们实际上不能只通过点图信息来进行建模（如纹理相同的两个不一样的平面或不同的一块地面），因此我们选择将特征与I2P网络中的特征融合：
$$
\mathcal{F}_i^{(T\times d)}=F_i^{(T\times d)}+\mathcal{P}_i^{(T\times d)},i=1,...,K+1.
$$
在这之后，我们便生成了每张点图的位置外观特征序列。&lt;/p&gt;
&lt;p&gt;紧接着，我们会这$K + 1$个点图输入到两个解码器中：&lt;/p&gt;
&lt;h4&gt;Registration Decoder&lt;/h4&gt;
&lt;p&gt;Registration Decoder将所有token作为输入，然后目的是将L2W的关键帧重建转换到场景坐标系下，它与$D_{key}$采用相同的架构。&lt;/p&gt;
&lt;p&gt;解码过程大概是：
$$
\mathcal{G}&lt;em&gt;{sce_i}=D&lt;/em&gt;{sce}(\mathcal{F}&lt;em&gt;{sce_i},\mathcal{F}&lt;/em&gt;{key}),\quad i=1,...,K
$$&lt;/p&gt;
&lt;h4&gt;Scene Decoder&lt;/h4&gt;
&lt;p&gt;Scene Decoder同样将所有token作为输入，但是它的目的是在不改变场景坐标系的情况下，精化坐标几何。他同样采用与$D_{key}$相同的架构，但是他是对每一个在已选中的关键帧点图进行优化：
$$
\mathcal{G}&lt;em&gt;{sce_i}=D&lt;/em&gt;{sce}(\mathcal{F}&lt;em&gt;{sce_i},\mathcal{F}&lt;/em&gt;{key}),\quad i=1,...,K
$$
通过这样的方式将已生成的point map进行优化&lt;/p&gt;
&lt;p&gt;最后，我们采用类似于I2P模块中的方法对我们所有已经重建的关键帧token进行点图重建：
$$
\tilde{X}_i^{(H\times W\times3)},\tilde{C}_i^{(H\times W\times1)}=\mathrm{H}(\mathcal{G}_i^{(T\times d)}),i=1,...,K+1.
$$&lt;/p&gt;
&lt;p&gt;得到一个实时的三维表示。&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;本人目前涉猎不深，但是论文最后与其他系统做比较，其展现的效率确实令我印象深刻，感觉以上的这个系统的两大模块也令非常简洁舒适。等我再去阅读其他的3R文章来进一步理解这个SOTA的含金量吧😋&lt;/p&gt;
&lt;p&gt;github项目地址：&lt;/p&gt;
&lt;p&gt;import { GithubCard } from &apos;astro-pure/advanced&apos;&lt;/p&gt;
&lt;p&gt;喵喵又是充实的一天🥳，本人可能理解有偏差（bushi&lt;/p&gt;</content:encoded><h:img src="/_astro/cover.uwodCb2H.png"/><enclosure url="/_astro/cover.uwodCb2H.png"/></item><item><title>Celebrate and Introduce My First Page</title><link>https://hjcheng0602.github.io/blog/celebrate</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/celebrate</guid><description>Just celebrate this page as a milestone and introduce the future of the site</description><pubDate>Sat, 02 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Here, I build my first &lt;a href=&quot;https://hjcheng0602.github.io/&quot;&gt;website&lt;/a&gt;(not the first but the first one I&apos;m serious about building/running)😋.&lt;/p&gt;
&lt;p&gt;My website will include:&lt;/p&gt;
&lt;h2&gt;study course expriences&lt;/h2&gt;
&lt;p&gt;This kind of content will record my experiences learning some meaningful courses in PKU.I hope it will help me review my courses.&lt;/p&gt;
&lt;h2&gt;research experiences&lt;/h2&gt;
&lt;p&gt;As a college student, researching and finding will be the main task in the future. Currently I am interested in 3R(3D reconstruction). So maybe I will update huge contents about my reflections for each paper.&lt;/p&gt;
&lt;h2&gt;my own projects&lt;/h2&gt;
&lt;p&gt;Of course, my some great(just in my standard) project will be post on the site. It&apos;s meaningful to me as long as I think it&apos;s great, regardless of how others see it.&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;Above might be the main topics of content in the site.&lt;/p&gt;
&lt;h3&gt;Additions&lt;/h3&gt;
&lt;p&gt;The posts will be in Chinese and English randomly(maybe most time Chinese🤣).Please forgive my poor English.&lt;/p&gt;</content:encoded><h:img src="undefined"/><enclosure url="undefined"/></item><item><title>Mast3R都之后的发现</title><link>https://hjcheng0602.github.io/blog/mst3r/mast3r</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/mst3r/mast3r</guid><description>Markdown 是一种轻量级的「标记语言」。</description><pubDate>Wed, 26 Jul 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;111&lt;/p&gt;</content:encoded><h:img src="undefined"/><enclosure url="undefined"/></item></channel></rss>