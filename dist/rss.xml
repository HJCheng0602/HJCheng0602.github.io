<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>Two and One</title><description>learn like a kid.</description><link>https://hjcheng0602.github.io</link><item><title>VGGTè¯»åæœ‰æ„Ÿ</title><link>https://hjcheng0602.github.io/blog/vggt/vggt</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/vggt/vggt</guid><description>æœ¬äººè¯»å®ŒVGGT:Visual Geometry Grounded Transformerä¹‹åçš„æ„Ÿæƒ³å–µ</description><pubDate>Thu, 14 Aug 2025 20:04:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Spoiler } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;h2&gt;å¼•è¨€&lt;/h2&gt;
&lt;p&gt;ç»§å†™å®Œ&lt;strong&gt;SLAM3R&lt;/strong&gt;çš„onlineeå¤„ç†åï¼Œæˆ‘åˆå°†ç›®å…‰æŠ•å‘äº†ä»Šå¹´CVPRçš„æœ€ä½³è®ºæ–‡ï¼š&lt;a href=&quot;https://github.com/facebookresearch/vggt&quot;&gt;VGGT:Visual Geometry Grounded Transformer&lt;/a&gt; ä¸è¦é—®æˆ‘ç ”ç©¶3Rä¸ºä»€ä¹ˆä¸å…ˆçœ‹vggtğŸ˜‚,é—®å°±æ˜¯æˆ‘å¤ªæ‘†äº†ä¸€å¼€å§‹æ‡’å¾—çœ‹äº†ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VGGT&lt;/strong&gt;ä¸»è¦ä»‹ç»äº†ä¸€ä¸ªç¦»çº¿çš„å¤šè§†å›¾é‡å»ºï¼Œä½å§¿ä¼°è®¡å’Œè½¨è¿¹è¿½è¸ªçš„å¼ºå¤§çš„æ¨¡å‹ï¼Œä¸ä¹‹å‰ç±»ä¼¼äº&lt;em&gt;SfM&lt;/em&gt;ã€&lt;strong&gt;DUst3R&lt;/strong&gt;çš„é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒçš„å…ˆè¿›ä¹‹å¤„åœ¨äºï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ‘†è„±äº†è¿™äº›æ–¹æ³•æ‰€ä¾èµ–çš„æ˜‚è´µçš„åå¤„ç†è¿‡ç¨‹ï¼ˆè€Œè¿™é€šå¸¸æ²¡æœ‰è®¡å…¥åˆ°ä¹‹å‰æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ä¸­ï¼‰&lt;/li&gt;
&lt;li&gt;å°†å¤šä¸ªä»»åŠ¡ï¼šæ·±åº¦ä¼°è®¡ã€ä½å§¿ä¼°è®¡ã€è§†å›¾é‡å»ºã€è½¨è¿¹è¿½è¸ªç­‰å…¨éƒ¨è¾“å‡ºï¼Œè¡¨ç°ç”šè‡³è¶…è¿‡äº†ä¹‹å‰å•ä¸€é¢†åŸŸçš„&lt;strong&gt;SOTA&lt;/strong&gt;æ–¹æ³•ã€‚&lt;/li&gt;
&lt;li&gt;åœ¨å°†å¤šä¸ªä»»åŠ¡çš„ç»“æœå…¨éƒ¨è¾“å‡ºçš„è¿‡ç¨‹ä¸­ï¼Œä½œè€…å‘ç°äº†å¼•å…¥ä¸åŒç»“æœä¹‹é—´çš„å†…åœ¨æ•°å­¦è”ç³»é™åˆ¶åä¼šå¤§å¹…æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;é¡¹ç›®æ¶æ„&lt;/h2&gt;
&lt;p&gt;ä¸ä¹‹å‰çš„æ¨¡å—åŒ–è§£å†³é—®é¢˜ä¸åŒï¼Œ&lt;strong&gt;VGGT&lt;/strong&gt;çš„ä¸»è¦ç»“æ„æ˜¯ä¸€ä¸ªå¤§çš„Transformerï¼Œå®ƒæ¥å—ä¸€ä¸ªå›¾ç‰‡é›†ä½œä¸ºè¾“å…¥ï¼Œç„¶åè¾“å‡ºåœºæ™¯å›¾ç‰‡çš„ä¸åŒä¸‰ç»´å±æ€§ã€‚&lt;/p&gt;
&lt;p&gt;å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒæ‰€èƒ½è§£å†³çš„å¤šè§†è§’ä¸‰ç»´å±æ€§å‡ ä¹æ¶µç›–äº†ä¸‰ç»´è§†è§‰çš„æ–¹æ–¹é¢é¢ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç›¸æœºä½å§¿ä»¥åŠå†…å‚&lt;/li&gt;
&lt;li&gt;ç‚¹å›¾é‡å»º&lt;/li&gt;
&lt;li&gt;å…³é”®åŒºåŸŸè¿½è¸ª&lt;/li&gt;
&lt;li&gt;å…³äºå•å¼ å›¾ç‰‡çš„æ·±åº¦å›¾&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¹¶ä¸”ï¼ŒVGGTé€šè¿‡æ›´åŠ åˆ›æ–°çš„ä¸¾åŠ¨ï¼Œå®ƒå°†è¾“å‡ºçš„å¤šä»»åŠ¡æˆæœçš„å†…åœ¨å‡ ä½•å…³ç³»ä½œä¸ºå½’çº³åç½®æ•´åˆè¿›äº†æ¨¡å‹ï¼Œå¹¶å‘ç°äº†å¤§å¹…åº¦çš„æ€§èƒ½æå‡ï¼Œè¿™ä¸ªå¾ˆå€¼å¾—å»ç ”ç©¶ã€‚&lt;/p&gt;
&lt;h2&gt;æ€»ç»“&lt;/h2&gt;
&lt;p&gt;æ„Ÿè§‰&lt;strong&gt;VGGT&lt;/strong&gt;å°±æ˜¯ä¸€ä¸ªå·¨å¤§çš„transformerï¼Œé€šè¿‡æå…¶æš´åŠ›çš„æ‰‹æ®µè§£å†³é—®é¢˜ï¼Œå®¢è§‚ä¸Šæ¥è¯´ï¼Œè¿™ç¡®å®å±•ç¤ºäº†transformeråœ¨ä¸‰ç»´é‡å»ºé¢†åŸŸçš„åº”ç”¨ï¼Œä½†å…¶å®æˆ‘æ˜¯æœ‰ä¸€äº›ç–‘é—®çš„ï¼š
åƒè‡ªç„¶è¯­è¨€å¤„ç†è¿™ç§å·¥ä½œï¼Œå®ƒæ˜¯æ— æ³•å®šé‡åŒ–å»ç ”ç©¶çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥äº†transformerï¼Œä¼¼ä¹æ˜¯ç”¨æœªçŸ¥å¯¹æŠ—ä¸ç¡®å®šæ€§çš„æ‰‹æ®µï¼Œä½†æ˜¯ï¼Œåœ¨è¿™ä¸ªä¸‰ç»´é‡å»ºè¿™ä¸ªé¢†åŸŸï¼Œå®ƒçœŸçš„æœ‰é‚£ä¹ˆå¤šä¸ç¡®å®šæ€§å—ï¼Ÿ
è¿˜æ˜¯æ„Ÿè§‰transformerå¯¹äºä¸‰ç»´é‡å»ºçš„æˆæœå±äºæ˜¯ç»“æœèƒ½çœ‹ï¼Œä½†æ˜¯è¦è¾¾åˆ°æ›´é«˜çš„ç²¾åº¦ä¼šè®©äººå¾ˆè¿·æƒ‘ã€‚&lt;/p&gt;</content:encoded><h:img src="/_astro/image.CZ_2N9gA.png"/><enclosure url="/_astro/image.CZ_2N9gA.png"/></item><item><title>ä¸ºSLAM3Rè¡¥å……å®æ—¶å¤„ç†å‡½æ•°æ–¹æ³•</title><link>https://hjcheng0602.github.io/blog/slam3r_online-edit/slam3r_online_contribute</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/slam3r_online-edit/slam3r_online_contribute</guid><description>åŸæœ¬çš„SLAM3Rçš„recon.pyçš„å¤„ç†é¡ºåºæ˜¯ä¸€ä¸ªofflineçš„é€»è¾‘ï¼Œå°†å…¶æ·»åŠ äº†onlineå¤„ç†çš„recon_online.py</description><pubDate>Tue, 12 Aug 2025 15:57:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Spoiler } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;p&gt;åœ¨ä¸Šä¸ªå‘¨é˜…è¯»&lt;strong&gt;SLAM3R&lt;/strong&gt;è®ºæ–‡ç»“æŸåï¼Œå­¦é•¿è®©æˆ‘å»çœ‹ä¸€ä¸‹å®ƒçš„&lt;a href=&quot;https://github.com/PKU-VCL-3DV/SLAM3R&quot;&gt;æºä»£ç &lt;/a&gt;ï¼Œè¯»å®Œä»£ç ä¹‹åï¼Œå‘ç°è™½ç„¶è®ºæ–‡é‡Œè®²è¿°çš„æ˜¯â€œå¯ä»¥å®æ—¶é‡å»ºâ€ï¼Œä½†æ˜¯å®é™…ä¸Šåœ¨&lt;code&gt;recon.py&lt;/code&gt;æ–‡ä»¶ä¸­çš„&lt;code&gt;scene_recon_pipeline&lt;/code&gt;å‡½æ•°ä¸­ï¼Œä»£ç é‡‡å–äº†å…ˆå¯¹æ‰€æœ‰&lt;code&gt;input_views&lt;/code&gt;è¿›è¡Œè¾“å…¥åˆ°&lt;code&gt;i2p_model&lt;/code&gt;å¾—åˆ°&lt;code&gt;res_feats&lt;/code&gt;ï¼Œç„¶åå†å°†æ‰€æœ‰å›¾ç‰‡çš„tokenè¾“å…¥åˆ°l2wç½‘ç»œä¸­è¿›è¡Œé‡å»ºçš„å¤§è‡´é€»è¾‘ã€‚&lt;/p&gt;
&lt;p&gt;æ˜¾ç„¶ï¼Œè¿™æ ·çš„å¤„ç†æ–¹æ³•ä¸æ˜¯è®ºæ–‡é‡Œæ‰€æå‡ºçš„&lt;strong&gt;online&lt;/strong&gt;å¤„ç†æ–¹æ³•ï¼Œå› æ­¤ï¼Œåœ¨è¿‡å»çš„ä¸€ä¸ªå‘¨é‡Œï¼Œæœ¬äººä¸€è¾¹ç»ƒç€ç§‘ä¸‰æ˜¾ç„¶ä»Šå¤©ä¸ŠåˆåˆšæŒ‚æ‰ï¼Œè¯¥æ­»çš„ç›´çº¿è¡Œé©¶ğŸ˜¡ï¼ŒåŒæ—¶æŠ½å‡ºäº†ä¸€ç‚¹ç‚¹æ—¶é—´å®Œæˆäº†&lt;code&gt;recon_online.py&lt;/code&gt;,ä¸€ä¸ªæŠŠåŸæœ¬çš„&lt;code&gt;scene_recon_pipeline&lt;/code&gt;æ”¹æˆ&lt;code&gt;online&lt;/code&gt;å¤„ç†çš„æ”¹åŠ¨ã€‚&lt;/p&gt;
&lt;h2&gt;åŸå‡½æ•°çš„å¤„ç†é€»è¾‘&lt;/h2&gt;
&lt;p&gt;é˜…è¯»åŸå‡½æ•°çš„ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†ä¸ºä»¥ä¸‹å‡ æ®µï¼š&lt;/p&gt;
&lt;h3&gt;é¢„å¤„ç†&amp;#x26;å¾—åˆ°æ‰€æœ‰viewçš„token&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Pre-save the RGB images along with their corresponding masks 
# in preparation for visualization at last.
rgb_imgs = []
for i in range(len(data_views)):
    if data_views[i][&apos;img&apos;].shape[0] == 1:
        data_views[i][&apos;img&apos;] = data_views[i][&apos;img&apos;][0]        
    rgb_imgs.append(transform_img(dict(img=data_views[i][&apos;img&apos;][None]))[...,::-1])
if &apos;valid_mask&apos; not in data_views[0]:
    valid_masks = None
else:
    valid_masks = [view[&apos;valid_mask&apos;] for view in data_views]   

#preprocess data for extracting their img tokens with encoder
for view in data_views:
    view[&apos;img&apos;] = torch.tensor(view[&apos;img&apos;][None])
    view[&apos;true_shape&apos;] = torch.tensor(view[&apos;true_shape&apos;][None])
    for key in [&apos;valid_mask&apos;, &apos;pts3d_cam&apos;, &apos;pts3d&apos;]:
        if key in view:
            del view[key]
    to_device(view, device=args.device)
# pre-extract img tokens by encoder, which can be reused 
# in the following inference by both i2p and l2w models
res_shapes, res_feats, res_poses = get_img_tokens(data_views, i2p_model)    # 300+fps
print(&apos;finish pre-extracting img tokens&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿™é‡Œé‡ç‚¹å°±æ˜¯æœ€åçš„&lt;code&gt;res_shapes, res_feats, res_poses = get_img_tokens(data_views, i2p_model)&lt;/code&gt;ï¼Œé‡‡ç”¨&lt;code&gt;i2p_model&lt;/code&gt;çš„&lt;code&gt;_encode_multiview&lt;/code&gt;æ–¹æ³•æ‰¹æ¬¡åŒ–åœ°(&lt;em&gt;batchify&lt;/em&gt;)å¯¹&lt;code&gt;data_views&lt;/code&gt;è¿›è¡Œå¤„ç†ï¼Œä»è€Œå¾—åˆ°æ‰€æœ‰çš„viewçš„&lt;code&gt;token&lt;/code&gt;ã€‚&lt;/p&gt;
&lt;h3&gt;å¯¹æ‰€æœ‰viewè¿›è¡Œæ¨ç†å¾—åˆ°æœ€åˆé€‚çš„key_frame_stride&lt;/h3&gt;
&lt;p&gt;è¿™é‡Œçš„æ ¸å¿ƒä»£ç å°±æ˜¯ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# decide the stride of sampling keyframes, as well as other related parameters
if args.keyframe_stride == -1:
    kf_stride = adapt_keyframe_stride(input_views, i2p_model, 
                                        win_r = 3,
                                        adapt_min=args.keyframe_adapt_min,
                                        adapt_max=args.keyframe_adapt_max,
                                        adapt_stride=args.keyframe_adapt_stride)
else:
    kf_stride = args.keyframe_stride
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å…¶ä¸­ï¼Œ&lt;code&gt;adapt_keyframe_stride&lt;/code&gt;å‡½æ•°æ˜¯ä¸€ä¸ªå…¸å‹çš„&lt;strong&gt;offline&lt;/strong&gt;å¤„ç†å‡½æ•°ï¼Œå®ƒçš„åŠŸèƒ½æ˜¯åœ¨æ‰€æœ‰çš„input_viewä¸­éå†å¯èƒ½çš„&lt;code&gt;kf_stride&lt;/code&gt;å–å€¼ï¼Œç„¶åå¯¹æ¯ä¸€ä¸ªå¯èƒ½çš„å–å€¼éšæœºå–æ ·ï¼Œç„¶ååˆ©ç”¨&lt;code&gt;i2p_inference_batch&lt;/code&gt;å‡½æ•°å¾—å‡ºç½®ä¿¡åº¦ä½œä¸ºç›¸ä¼¼åº¦ï¼Ÿç„¶åé€‰å–æœ€é«˜çš„æ‰€å¯¹åº”çš„&lt;code&gt;kf_stride&lt;/code&gt;ä½œä¸ºæœ€ä¼˜çš„å–å€¼ã€‚&lt;/p&gt;
&lt;h3&gt;ä½¿ç”¨åˆå§‹çš„å‡ ä¸ªæ»‘åŠ¨çª—å£åˆ›å»ºåˆå§‹çš„å…¨å±€scene&amp;#x26;åˆå§‹åŒ–buffer set&lt;/h3&gt;
&lt;p&gt;å› ä¸º&lt;strong&gt;SLAM3R&lt;/strong&gt;åˆå§‹åŒ–æ—¶çš„&lt;a href=&quot;http://localhost:4321/blog/slam3r/slam3r&quot;&gt;ç‰¹æ®Šæ€§&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;å¯¹äºç¬¬ä¸€ä¸ªå¸§è¿™ç§ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é‡å¤è¿è¡Œå¤šæ¬¡I2Pè·å–è¶³å¤Ÿå¤šæ•°é‡çš„åˆå§‹å¸§ä½œä¸ºç¼“å†²é›†&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;åœ¨åŸæœ¬çš„offlineæ ¼å¼çš„&lt;code&gt;recon.py&lt;/code&gt;ä¸­ï¼Œè¿™ç§åšæ³•ä»¥è¿™ç§æ ·å¼å‘ˆç°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride], 
                                                i2p_model, 
                                                winsize=initial_winsize,
                                                return_ref_id=True) # 5*(1,224,224,3)

# start reconstrution of the whole scene
init_num = len(initial_pcds)
per_frame_res = dict(i2p_pcds=[], i2p_confs=[], l2w_pcds=[], l2w_confs=[])
for key in per_frame_res:
    per_frame_res[key] = [None for _ in range(num_views)]

registered_confs_mean = [_ for _ in range(num_views)]

# set up the world coordinates with the initial window
for i in range(init_num):
    per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] = initial_confs[i][0].to(args.device)  # 224,224
    registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()

# initialize the buffering set with the initial window
assert args.buffer_size &amp;#x3C;= 0 or args.buffer_size &gt;= init_num 
buffering_set_ids = [i*kf_stride for i in range(init_num)]

# set up the world coordinates with frames in the initial window
for i in range(init_num):
    input_views[i*kf_stride][&apos;pts3d_world&apos;] = initial_pcds[i]
    
initial_valid_masks = [conf &gt; conf_thres_i2p for conf in initial_confs] # 1,224,224
normed_pts = normalize_views([view[&apos;pts3d_world&apos;] for view in input_views[:init_num*kf_stride:kf_stride]],
                                            initial_valid_masks)
for i in range(init_num):
    input_views[i*kf_stride][&apos;pts3d_world&apos;] = normed_pts[i]
    # filter out points with low confidence
    input_views[i*kf_stride][&apos;pts3d_world&apos;][~initial_valid_masks[i]] = 0       
    per_frame_res[&apos;l2w_pcds&apos;][i*kf_stride] = normed_pts[i]  # 224,224,3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å…¶ä¸­ï¼Œ&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride], 
                                                   i2p_model, 
                                                   winsize=initial_winsize,
                                                   return_ref_id=True) # 5*(1,224,224,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿™ä¸€è¡Œæ˜¯å¯¹åˆå§‹åŒ–çš„å‡ ä¸ª&lt;code&gt;view_token&lt;/code&gt;è¿›è¡Œåœºæ™¯é‡å»ºï¼Œå¹¶é€‰å‡ºä¸€å¼€å§‹çš„&lt;code&gt;init_ref_id&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ç„¶åä¹‹åå°±æ˜¯æŠŠæ‰€æœ‰åˆå§‹åŒ–çš„å¸§æ”¾åˆ°&lt;code&gt;buffer_set&lt;/code&gt;é‡Œï¼Œç„¶åè¿›è¡Œä¸€äº›å½’ä¸€åŒ–å¤„ç†ã€‚&lt;/p&gt;
&lt;h3&gt;å¯¹åŸå§‹çš„viewå†ç»§ç»­è¿›è¡Œi2pé‡å»ºç‚¹å›¾&lt;/h3&gt;
&lt;p&gt;è¿™é‡Œæˆ‘ä»¬é‡æ–°éå†æ‰€æœ‰å›¾åƒï¼Œå¯¹åº”è®ºæ–‡é‡Œé¢é€šè¿‡&lt;code&gt;I2P&lt;/code&gt;çš„&lt;code&gt;decoder&lt;/code&gt;é‡å»ºæ‰€æœ‰&lt;code&gt;view&lt;/code&gt;çš„ç‚¹å›¾ã€‚æ­¤å¤–ï¼Œæ³¨æ„&lt;code&gt;initial window&lt;/code&gt;çš„å…³é”®å¸§å›¾ç‰‡åŸºæœ¬ä¸Šå·²ç»åœ¨ä¸Šé¢çš„åˆå§‹åŒ–ä¸­è¢«åˆ›å»ºå‡ºäº†ç‚¹å›¾ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©ç•¥è¿‡ä»–ä»¬ï¼Œåªå¯¹æ²¡æœ‰è¢«åˆ›å»ºç‚¹å›¾çš„å¸§è¿›è¡Œ&lt;code&gt;I2P&lt;/code&gt;å¤„ç†
ä»¥å¾—åˆ°ç‚¹å›¾ï¼Œç„¶åå°±é‡‡ç”¨è®ºæ–‡ä¸­çš„è¾“å…¥çª—å£å¤šä¸ªå¸§ï¼Œé‡å»ºæ¯ä¸ªå¸§çš„ç‚¹äº‘ä½œä¸º&lt;code&gt;L2W model&lt;/code&gt;çš„è¾“å…¥ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for view_id in tqdm(range(num_views), desc=&quot;I2P resonstruction&quot;):
    # skip the views in the initial window
    if view_id in buffering_set_ids:
        # trick to mark the keyframe in the initial window
        if view_id // kf_stride == init_ref_id:
            per_frame_res[&apos;i2p_pcds&apos;][view_id] = per_frame_res[&apos;l2w_pcds&apos;][view_id].cpu()
        else:
            per_frame_res[&apos;i2p_pcds&apos;][view_id] = torch.zeros_like(per_frame_res[&apos;l2w_pcds&apos;][view_id], device=&quot;cpu&quot;)
        per_frame_res[&apos;i2p_confs&apos;][view_id] = per_frame_res[&apos;l2w_confs&apos;][view_id].cpu()
        continue
    # construct the local window 
    sel_ids = [view_id]
    for i in range(1,win_r+1):
        if view_id-i*adj_distance &gt;= 0:
            sel_ids.append(view_id-i*adj_distance)
        if view_id+i*adj_distance &amp;#x3C; num_views:
            sel_ids.append(view_id+i*adj_distance)
    local_views = [input_views[id] for id in sel_ids]
    ref_id = 0 
    # recover points in the local window, and save the keyframe points and confs
    output = i2p_inference_batch([local_views], i2p_model, ref_id=ref_id, 
                                tocpu=False, unsqueeze=False)[&apos;preds&apos;]
    #save results of the i2p model
    per_frame_res[&apos;i2p_pcds&apos;][view_id] = output[ref_id][&apos;pts3d&apos;].cpu() # 1,224,224,3
    per_frame_res[&apos;i2p_confs&apos;][view_id] = output[ref_id][&apos;conf&apos;][0].cpu() # 224,224

    # construct the input for L2W model        
    input_views[view_id][&apos;pts3d_cam&apos;] = output[ref_id][&apos;pts3d&apos;] # 1,224,224,3
    valid_mask = output[ref_id][&apos;conf&apos;] &gt; conf_thres_i2p # 1,224,224
    input_views[view_id][&apos;pts3d_cam&apos;] = normalize_views([input_views[view_id][&apos;pts3d_cam&apos;]],
                                                [valid_mask])[0]
    input_views[view_id][&apos;pts3d_cam&apos;][~valid_mask] = 0 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;å¯¹åˆå§‹çª—å£éå…³é”®å¸§è¿›è¡Œæ³¨å†Œ&lt;/h3&gt;
&lt;p&gt;æ˜¾ç„¶æˆ‘ä»¬åœ¨ä¹‹å‰çš„åˆå§‹åŒ–åœºæ™¯ä¸­åªæ³¨å†Œäº†å…³é”®å¸§ï¼Œå› æ­¤æˆ‘ä»¬ç°åœ¨å¼€å§‹å¯¹éå…³é”®å¸§è¿›è¡Œæ³¨å†Œï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Special treatment: register the frames within the range of initial window with L2W model
# TODO: batchify
if kf_stride &gt; 1:
    max_conf_mean = -1
    for view_id in tqdm(range((init_num-1)*kf_stride), desc=&quot;pre-registering&quot;):  
        if view_id % kf_stride == 0:
            continue
        # construct the input for L2W model
        l2w_input_views = [input_views[view_id]] + [input_views[id] for id in buffering_set_ids]
        # (for defination of ref_ids, see the doc of l2w_model)
        output = l2w_inference(l2w_input_views, l2w_model, 
                                ref_ids=list(range(1,len(l2w_input_views))), 
                                device=args.device,
                                normalize=args.norm_input)
        
        # process the output of L2W model
        input_views[view_id][&apos;pts3d_world&apos;] = output[0][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
        conf_map = output[0][&apos;conf&apos;] # 1,224,224
        per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0] # 224,224
        registered_confs_mean[view_id] = conf_map.mean().cpu()
        per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
        
        if registered_confs_mean[view_id] &gt; max_conf_mean:
            max_conf_mean = registered_confs_mean[view_id]
    print(f&apos;finish aligning {(init_num-1)*kf_stride} head frames, with a max mean confidence of {max_conf_mean:.2f}&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿™é‡Œæ­£å¦‚æ³¨é‡Šæ‰€è¯´ï¼Œæ˜¯ä¸€ä¸ª&lt;strong&gt;Special treatment&lt;/strong&gt;ã€‚ä¹Ÿæ˜¯ä¸€ä¸ªç‰¹æ®Šæƒ…å†µå¤„ç†ã€‚&lt;/p&gt;
&lt;h4&gt;ç¼©æ”¾confs&lt;/h4&gt;
&lt;p&gt;æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬åªç”¨&lt;code&gt;l2w&lt;/code&gt;ç½‘ç»œå¯¹éå…³é”®å¸§è¿›è¡Œäº†ç½®ä¿¡åº¦é¢„æµ‹ï¼Œå…³é”®å¸§çš„ç½®ä¿¡åº¦æ˜¯ç”±ä¹‹å‰çš„&lt;code&gt;i2p&lt;/code&gt;ç½‘ç»œè¿›è¡Œé¢„æµ‹çš„ï¼Œä½œè€…åœ¨è¿™é‡Œä¸ºäº†æ§åˆ¶è®¡ç®—æˆæœ¬ï¼Œé€‰æ‹©ç›´æ¥å°†åè€…ä¹˜ä¸Šä¸€ä¸ªå¸¸æ•°å› å­è¿›è¡Œç¼©æ”¾ï¼Œå¤§è‡´åæ˜ å‡ºäº†åœºæ™¯çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# A problem is that the registered_confs_mean of the initial window is generated by I2P model,
# while the registered_confs_mean of the frames within the initial window is generated by L2W model,
# so there exists a gap. Here we try to align it.
max_initial_conf_mean = -1
for i in range(init_num):
    if registered_confs_mean[i*kf_stride] &gt; max_initial_conf_mean:
        max_initial_conf_mean = registered_confs_mean[i*kf_stride]
factor = max_conf_mean/max_initial_conf_mean
# print(f&apos;align register confidence with a factor {factor}&apos;)
for i in range(init_num):
    per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] *= factor
    registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;å¯¹å‰©ä¸‹çš„viewsè¿›è¡Œæ³¨å†Œ&lt;/h3&gt;
&lt;p&gt;OKï¼Œç»è¿‡äº†ä»¥ä¸Šçš„å¯¹äºåˆå§‹å¸§çš„ç‰¹æ®Šå¤„ç†ï¼Œæˆ‘ä»¬ç»ˆäºè¸å…¥äº†æ­£é€”ï¼šåœ¨è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå¸§è¿›è¡Œå®æ—¶å¤„ç†&lt;/p&gt;
&lt;h4&gt;ä»buffer seté‡Œé€‰æ‹©æœ€ç›¸è¿‘çš„sel_numä¸ªå¸§ï¼š&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# select sccene frames in the buffering set to work as a global reference
cand_ref_ids = buffering_set_ids
ref_views, sel_pool_ids = scene_frame_retrieve(
    [input_views[i] for i in cand_ref_ids], 
    input_views[ni:ni+num_register:2], 
    i2p_model, sel_num=num_scene_frame, 
    # cand_recon_confs=[per_frame_res[&apos;l2w_confs&apos;][i] for i in cand_ref_ids],
    depth=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿™é‡Œæ­£å¦‚è®ºæ–‡ä¸­æ‰€è¿°ï¼Œé‡‡ç”¨äº†&lt;code&gt;i2p_model&lt;/code&gt;çš„å‰2ä¸ª&lt;strong&gt;decoder&lt;/strong&gt;è¿›è¡Œç›¸ä¼¼è¯„åˆ†ã€‚&lt;/p&gt;
&lt;h4&gt;å°†é€‰å–çš„æœ€ç›¸è¿‘çš„å‡ ä¸ªå¸§ä½œä¸ºå‚è€ƒåˆå¹¶å½“å‰å¸§è¿›è¡Œl2wé‡å»º&lt;/h4&gt;
&lt;p&gt;æ˜¾è€Œæ˜“è§ï¼Œè¨€ä»¥æ¦‚ä¹‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# register the source frames in the local coordinates to the world coordinates with L2W model
l2w_input_views = ref_views + input_views[ni:max_id+1]
input_view_num = len(ref_views) + max_id - ni + 1
assert input_view_num == len(l2w_input_views)

output = l2w_inference(l2w_input_views, l2w_model, 
                        ref_ids=list(range(len(ref_views))), 
                        device=args.device,
                        normalize=args.norm_input)

# process the output of L2W model
src_ids_local = [id+len(ref_views) for id in range(max_id-ni+1)]  # the ids of src views in the local window
src_ids_global = [id for id in range(ni, max_id+1)]    #the ids of src views in the whole dataset
succ_num = 0
for id in range(len(src_ids_global)):
    output_id = src_ids_local[id] # the id of the output in the output list
    view_id = src_ids_global[id]    # the id of the view in all views
    conf_map = output[output_id][&apos;conf&apos;] # 1,224,224
    input_views[view_id][&apos;pts3d_world&apos;] = output[output_id][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
    per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0]
    registered_confs_mean[view_id] = conf_map[0].mean().cpu()
    per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
    succ_num += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;import { Aside } from &apos;astro-pure/user&apos;&lt;/p&gt;
&lt;h4&gt;é€šè¿‡ä¸€äº›æ‰‹æ®µæ›´æ–°buffer set&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;buffer_set&lt;/code&gt;çš„é€‰å–æ–¹æ³•å·®ä¸å¤šå°±å’Œè®ºæ–‡é‡Œé¢è®²çš„ä¸€æ ·ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯éšæœºé€‰å–äº†ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# update the buffering set
if next_register_id - milestone &gt;= update_buffer_intv:  
    while(next_register_id - milestone &gt;= kf_stride):
        candi_frame_id += 1
        full_flag = max_buffer_size &gt; 0 and len(buffering_set_ids) &gt;= max_buffer_size
        insert_flag = (not full_flag) or ((strategy == &apos;fifo&apos;) or 
                                            (strategy == &apos;reservoir&apos; and np.random.rand() &amp;#x3C; max_buffer_size/candi_frame_id))
        if not insert_flag: 
            milestone += kf_stride
            continue
        # Use offest to ensure the selected view is not too close to the last selected view
        # If the last selected view is 0, 
        # the next selected view should be at least kf_stride*3//4 frames away
        start_ids_offset = max(0, buffering_set_ids[-1]+kf_stride*3//4 - milestone)
            
        # get the mean confidence of the candidate views
        mean_cand_recon_confs = torch.stack([registered_confs_mean[i]
                                    for i in range(milestone+start_ids_offset, milestone+kf_stride)])
        mean_cand_local_confs = torch.stack([local_confs_mean[i]
                                    for i in range(milestone+start_ids_offset, milestone+kf_stride)])
        # normalize the confidence to [0,1], to avoid overconfidence
        mean_cand_recon_confs = (mean_cand_recon_confs - 1)/mean_cand_recon_confs # transform to sigmoid
        mean_cand_local_confs = (mean_cand_local_confs - 1)/mean_cand_local_confs
        # the final confidence is the product of the two kinds of confidences
        mean_cand_confs = mean_cand_recon_confs*mean_cand_local_confs
        
        most_conf_id = mean_cand_confs.argmax().item()
        most_conf_id += start_ids_offset
        id_to_buffer = milestone + most_conf_id
        buffering_set_ids.append(id_to_buffer)
        # print(f&quot;add ref view {id_to_buffer}&quot;)                
        # since we have inserted a new frame, overflow must happen when full_flag is True
        if full_flag:
            if strategy == &apos;reservoir&apos;:
                buffering_set_ids.pop(np.random.randint(max_buffer_size))
            elif strategy == &apos;fifo&apos;:
                buffering_set_ids.pop(0)
        # print(next_register_id, buffering_set_ids)
        milestone += kf_stride
# transfer the data to cpu if it is not in the buffering set, to save gpu memory
for i in range(next_register_id):
    to_device(input_views[i], device=args.device if i in buffering_set_ids else &apos;cpu&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;ä¿å­˜ç¯èŠ‚&lt;/h3&gt;
&lt;p&gt;å½“æˆ‘ä»¬å¤„ç†å®Œæ‰€æœ‰å¸§åï¼Œæˆ‘ä»¬ä¼šä¿å­˜æˆ‘ä»¬çš„æ‰€æœ‰å¸§çš„ç‚¹äº‘ï¼ŒæŠŠè¿™äº›æ‰€æœ‰å¸§çš„ç‚¹äº‘åˆåˆ°ä¸€èµ·è¿›è¡Œé‡å»ºï¼Œå¾—å‡ºæœ€åçš„åœºæ™¯ç‚¹äº‘ã€‚&lt;/p&gt;
&lt;h3&gt;review&lt;/h3&gt;
&lt;p&gt;æ˜¾è€Œæ˜“è§ï¼ŒåŸ&lt;code&gt;recon.py&lt;/code&gt;ä¸­çš„è¿™ä¸ª&lt;code&gt;pipeline&lt;/code&gt;æ˜¯ä¸€ä¸ªå®Œå…¨çš„&lt;strong&gt;offline&lt;/strong&gt;å¤„ç†æ–¹æ³•ï¼Œå› æ­¤ï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªçœŸæ­£çš„ï¼ˆï¼Ÿ&lt;strong&gt;online&lt;/strong&gt;ç‰ˆæœ¬çš„æ–¹æ³•ï¼Œå¤„ç†é€»è¾‘å¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/p&gt;
&lt;h2&gt;online å‡½æ•°çš„å¤„ç†é€»è¾‘&lt;/h2&gt;
&lt;p&gt;æ—¢ç„¶æ˜¯è¦onlineï¼Œæˆ‘ä»¬æ˜¾ç„¶ç¬¬ä¸€ä»¶è¦åšçš„äº‹æƒ…å°±æ˜¯å†™ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(len(data_views)):
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä¹‹åæˆ‘ä»¬åœ¨è¿›è¡Œä¸€ç³»åˆ—å¤„ç†ï¼š&lt;/p&gt;
&lt;h3&gt;é¢„å¤„ç† &amp;#x26; å¾—åˆ°å½“å‰viewçš„token&lt;/h3&gt;
&lt;p&gt;æ˜¾ç„¶ï¼Œé€šè¿‡å¯¹åŸå…ˆ&lt;strong&gt;offline&lt;/strong&gt;ç‰ˆæœ¬çš„å‡½æ•°åˆ†æï¼Œè¿™ä¸ªè¿‡ç¨‹æ²¡æœ‰åˆå§‹åŒ–çš„å›°æ‰°ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¤§èƒ†å¯¹æ‰€æœ‰éå†åˆ°çš„viewéƒ½è¿›è¡Œè¿™ä¸€æ­¥ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Pre-save the RGB images along with their corresponding masks
# in preparation for visualization at last.

if data_views[i][&apos;img&apos;].shape[0] == 1:
    data_views[i][&apos;img&apos;] = data_views[i][&apos;img&apos;][0]
rgb_imgs.append(transform_img(dict(img=data_views[i][&apos;img&apos;][None]))[...,::-1])

if is_have_mask_rgb:
    valid_masks.append(data_views[i][&apos;valid_mask&apos;])

# process now image for extracting its img token with encoder
data_views[i][&apos;img&apos;] = torch.tensor(data_views[i][&apos;img&apos;][None])
data_views[i][&apos;true_shape&apos;] = torch.tensor(data_views[i][&apos;true_shape&apos;][None])
for key in [&apos;valid_mask&apos;, &apos;pts3d_cam&apos;, &apos;pts3d&apos;]:
    if key in data_views[i]:
        del data_views[key]
to_device(data_views[i], device=args.device)

# pre-extract img tokens by encoder, which can be reused 
# in the following inference by both i2p and l2w models
temp_shape, temp_feat, temp_pose = get_single_img_tokens([data_views[i]], i2p_model, True)
res_shapes.append(temp_shape[0])
res_feats.append(temp_feat[0])
res_poses.append(temp_pose[0])
print(f&quot;finish pre-extracting img token of view {i}&quot;)

input_views.append(dict(label=data_views[i][&apos;label&apos;],
                        img_tokens=temp_feat[0],
                        true_shape=data_views[i][&apos;true_shape&apos;],
                        img_pos=temp_pose[0]))
for key in per_frame_res:
    per_frame_res[key].append(None)
registered_confs_mean.append(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿™é‡Œæˆ‘ä½¿ç”¨äº†ä¸€ä¸ª&lt;code&gt;get_single_img_tokens&lt;/code&gt;å‡½æ•°ï¼Œä¸ä¹‹å‰çš„&lt;code&gt;get_img_tokens&lt;/code&gt;å‡½æ•°ç›¸æ¯”ï¼Œè¯¥å‡½æ•°é™¤äº†ä¸èƒ½batchåŒ–(onlineçš„é™åˆ¶)ä¹‹å¤–ï¼Œæ•ˆæœè¾“å‡ºåˆ«æ— äºŒè‡´ã€‚&lt;/p&gt;
&lt;h3&gt;ç§¯ç´¯å¸§ä»¥ç”¨äºåœºæ™¯åˆå§‹åŒ–&lt;/h3&gt;
&lt;p&gt;éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“å¸§åºæ•°å°äºåˆå§‹åŒ–æ‰€éœ€è¦çš„å¸§æ•°æ—¶ï¼Œæˆ‘ä»¬åç»­çš„ç¨‹åºå‡æ— æ³•è¿›è¡Œï¼Œå› æ­¤åœ¨æˆ‘çš„ä»£ç ä¸­ï¼Œæˆ‘é€‰æ‹©ç›´æ¥è·³è¿‡ï¼Œå…ˆè“„åŠ¿å¾…å‘ğŸ¤£&lt;/p&gt;
&lt;p&gt;ä¸€æ—¦ç§¯ç´¯åˆ°åˆå§‹åŒ–åœºæ™¯æ‰€éœ€å¸§åï¼Œå‡½æ•°ä¼šé‡‡ç”¨ä¸€ç³»åˆ—æ“ä½œåˆå§‹åŒ–åœºæ™¯ä»¥åŠåˆå§‹åŒ–buffer setï¼Œå¯¹åˆå§‹åŒ–åçš„å„å¸§ç‚¹äº‘è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# accumulate the initial window frames
if i &amp;#x3C; (initial_winsize - 1)*kf_stride and i % kf_stride == 0:
    continue
elif i == (initial_winsize - 1)*kf_stride:
    initial_pcds, initial_confs, init_ref_id = initialize_scene(input_views[:initial_winsize*kf_stride:kf_stride],
                                                                i2p_model,
                                                                winsize=initial_winsize,
                                                                return_ref_id=True)
    # set up the world coordinates with the initial window
    init_num = len(initial_pcds)
    for j in range(init_num):
        per_frame_res[&apos;l2w_confs&apos;][j * kf_stride] = initial_confs[j][0].to(args.device)
        registered_confs_mean[j * kf_stride] = per_frame_res[&apos;l2w_confs&apos;][j * kf_stride].mean().cpu()
    # initialize the buffering set with the initial window
    assert args.buffer_size &amp;#x3C;= 0 or args.buffer_size &gt;= init_num 
    buffering_set_ids = [j*kf_stride for j in range(init_num)]
    # set ip the woeld coordinates with frames in the initial window
    for j in range(init_num):
        input_views[j*kf_stride][&apos;pts3d_world&apos;] = initial_pcds[j]
    initial_valid_masks = [conf &gt; conf_thres_i2p for conf in initial_confs]
    normed_pts = normalize_views([view[&apos;pts3d_world&apos;] for view in input_views[:init_num*kf_stride:kf_stride]],
                                                initial_valid_masks)
    for j in range(init_num):
        input_views[j*kf_stride][&apos;pts3d_world&apos;] = normed_pts[j]
        # filter out points with low confidence
        input_views[j*kf_stride][&apos;pts3d_world&apos;][~initial_valid_masks[j]] = 0
        per_frame_res[&apos;l2w_pcds&apos;][j*kf_stride] = normed_pts[j]

elif i &amp;#x3C; (initial_winsize - 1) * kf_stride:
    continue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œä¸€æ—¦ç§¯ç´¯åˆ°è¶³å¤Ÿå¤šçš„åˆå§‹å¸§ï¼Œæˆ‘ä»¬å°±ä¸ä¼šè¿›è¡Œcontinueå¤„ç†äº†ï¼Œç„¶åç›´æ¥è¿›è¡Œä¸‹ä¸€éƒ¨åˆ†ã€‚&lt;/p&gt;
&lt;h3&gt;å¯¹ä¹‹å‰ç§¯ç´¯çš„viewè¿›è¡Œi2pé‡å»ºç‚¹å›¾ï¼ˆåŒ…å«æ­£åœ¨å¤„ç†çš„å¸§ï¼‰ &amp;#x26; æ³¨å†Œåˆå§‹çª—å£éå…³é”®å¸§&lt;/h3&gt;
&lt;p&gt;è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ç±»ä¼¼äºä¹‹å‰&lt;strong&gt;offline&lt;/strong&gt;çš„é¡ºåºï¼Œåªä¸è¿‡æŠŠå¤–åœ¨çš„è¡¨ç°å½¢å¼ä½œå‡ºäº†æ”¹å˜ï¼Œå®é™…ä¸Šå†…åœ¨çš„é¡ºåºé€»è¾‘åŸºæœ¬ä¸å˜ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# first recover the accumulate views
if i == (initial_winsize - 1) * kf_stride:
    for view_id in range(i + 1):
        # skip the views in the initial window
        if view_id in buffering_set_ids:
            # trick to mark the keyframe in the initial window
            if view_id // kf_stride == init_ref_id:
                per_frame_res[&apos;i2p_pcds&apos;][view_id] = per_frame_res[&apos;l2w_pcds&apos;][view_id].cpu()
            else:
                per_frame_res[&apos;i2p_pcds&apos;][view_id] = torch.zeros_like(per_frame_res[&apos;l2w_pcds&apos;][view_id], device=&quot;cpu&quot;)
            per_frame_res[&apos;i2p_confs&apos;][view_id] = per_frame_res[&apos;l2w_confs&apos;][view_id].cpu()
            print(f&quot;finish revocer pcd of frame {view_id} in their local coordinates(in buffer set), with a mean confidence of {per_frame_res[&apos;i2p_confs&apos;][view_id].mean():.2f} up to now.&quot;)
            continue
        # construct the local window with the initial views
        sel_ids = [view_id]
        for j in range(1, win_r + 1):
            if view_id - j * adj_distance &gt;= 0:
                sel_ids.append(view_id - j * adj_distance)
            if view_id + j * adj_distance &amp;#x3C; i:
                sel_ids.append(view_id + j * adj_distance)
        local_views = [input_views[id] for id in sel_ids]
        ref_id = 0

        # recover poionts in the initial window, and save the keyframe points and confs
        output = i2p_inference_batch([local_views], i2p_model, ref_id=ref_id,
                                        tocpu=False, unsqueeze=False)[&apos;preds&apos;]
        # save results of the i2p model for the initial window
        per_frame_res[&apos;i2p_pcds&apos;][view_id] = output[ref_id][&apos;pts3d&apos;].cpu()
        per_frame_res[&apos;i2p_confs&apos;][view_id] = output[ref_id][&apos;conf&apos;][0].cpu()

        # construct the input for L2W model
        input_views[view_id][&apos;pts3d_cam&apos;] = output[ref_id][&apos;pts3d&apos;]
        valid_mask = output[ref_id][&apos;conf&apos;] &gt; conf_thres_i2p
        input_views[view_id][&apos;pts3d_cam&apos;] = normalize_views([input_views[view_id][&apos;pts3d_cam&apos;]],
                                                                [valid_mask])[0]
        input_views[view_id][&apos;pts3d_cam&apos;][~valid_mask] = 0

        local_confs_mean_up2now = [conf.mean() for conf in per_frame_res[&apos;i2p_confs&apos;] if conf is not None]
        print(f&quot;finish revocer pcd of frame {view_id} in their local coordinates, with a mean confidence of {torch.stack(local_confs_mean_up2now).mean():.2f} up to now.&quot;)

    # Special treatment: register the frames within the range of initial window with L2W model
    if kf_stride &gt; 1:
        max_conf_mean = -1
        for view_id in tqdm(range((init_num - 1) * kf_stride), desc=&quot;pre-registering&quot;):
            if view_id % kf_stride == 0:
                continue
            # construct the input for L2W model

            l2w_input_views = [input_views[view_id]] + [input_views[id] for id in buffering_set_ids]
            # (for defination of ref_ids, seee the doc of l2w_model)
            output = l2w_inference(l2w_input_views, l2w_model,
                                    ref_ids=list(range(1,len(l2w_input_views))),
                                    device=args.device,
                                    normalize=args.norm_input)
            # process the output of L2W model
            input_views[view_id][&apos;pts3d_world&apos;] = output[0][&apos;pts3d_in_other_view&apos;] # 1,224,224,3
            conf_map = output[0][&apos;conf&apos;] # 1,224,224
            per_frame_res[&apos;l2w_confs&apos;][view_id] = conf_map[0] # 224,224
            registered_confs_mean[view_id] = conf_map.mean().cpu()
            per_frame_res[&apos;l2w_pcds&apos;][view_id] = input_views[view_id][&apos;pts3d_world&apos;]
            
            if registered_confs_mean[view_id] &gt; max_conf_mean:
                max_conf_mean = registered_confs_mean[view_id]
        print(f&apos;finish aligning {(init_num)*kf_stride} head frames, with a max mean confidence of {max_conf_mean:.2f}&apos;)
        # A problem is that the registered_confs_mean of the initial window is generated by I2P model,
        # while the registered_confs_mean of the frames within the initial window is generated by L2W model,
        # so there exists a gap. Here we try to align it.
        max_initial_conf_mean = -1
        for i in range(init_num):
            if registered_confs_mean[i*kf_stride] &gt; max_initial_conf_mean:
                max_initial_conf_mean = registered_confs_mean[i*kf_stride]
        factor = max_conf_mean/max_initial_conf_mean
        # print(f&apos;align register confidence with a factor {factor}&apos;)
        for i in range(init_num):
            per_frame_res[&apos;l2w_confs&apos;][i*kf_stride] *= factor
            registered_confs_mean[i*kf_stride] = per_frame_res[&apos;l2w_confs&apos;][i*kf_stride].mean().cpu()
    # register the rest frames with L2W model
    next_register_id = (init_num - 1) * kf_stride + 1
    milestone = init_num * kf_stride + 1
    update_buffer_intv = kf_stride*args.update_buffer_intv   # update the buffering set every update_buffer_intv frames
    max_buffer_size = args.buffer_size
    strategy = args.buffer_strategy
    candi_frame_id = len(buffering_set_ids) # used for the reservoir sampling strategy
    continue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç„¶ååœ¨å¤„ç†å®Œè¿™ä¹ˆä¸€å †ä¹‹åæˆ‘ä»¬ç›´æ¥&lt;code&gt;continue&lt;/code&gt;åˆ°ä¸‹ä¸€ä¸ªå¾ªç¯ã€‚&lt;/p&gt;
&lt;h3&gt;å¤„ç†æ–°å›¾ç‰‡&lt;/h3&gt;
&lt;p&gt;åœ¨ä¸‹ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬æ‹¿åˆ°äº†æ–°å›¾ç‰‡ï¼Œæ­¤æ—¶æˆ‘ä»¬ä¹Ÿåœ¨æˆ‘ä»¬çš„&lt;strong&gt;online&lt;/strong&gt;å‡½æ•°ä¸­è¸ä¸Šäº†æ­£é€”ï¼Œå¯ä»¥å¯¹æ¯ä¸€ä¸ªå¸§è¿›è¡Œå®æ—¶å¤„ç†äº†ã€‚&lt;/p&gt;
&lt;p&gt;è¿™é‡Œï¼Œæˆ‘ä»¬çš„å¤„ç†é€»è¾‘ä¸ç¬¬ä¸€ç§æ–¹æ³•ç±»ä¼¼ï¼Œä¸åŒçš„ä¸€ç‚¹æ˜¯æˆ‘æ˜¯ä¸€å¸§ä¸€å¸§åœ°å»å¤„ç†ã€‚&lt;/p&gt;
&lt;h3&gt;ä¿å­˜ç¯èŠ‚&lt;/h3&gt;
&lt;p&gt;ä¸ä¸Šä¸€ä¸ªæ–¹æ³•ç•¥å¾®ä¸åŒï¼Œæˆ‘æä¾›äº†å‚æ•°é€‰é¡¹é€‰æ‹©æ˜¯å¦åœ¨çº¿ä¿å­˜/é€å‡ å¸§ä¿å­˜ï¼Œå› æ­¤æˆ‘é‡å†™äº†ä¸€ä¸ªå¢é‡å¼ä¿å­˜çš„ç±»ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class IncrementalReconstructor:
    &quot;&quot;&quot;
    A class used for reconstruting the pts incrementally
    &quot;&quot;&quot;
    def __init__(self):
        self.res_pcds = None
        self.res_rgbs = None
        self.res_confs = None
        self.res_valid_masks = None
        self.is_initialized = False

    def add_frame(self, view: dict, img: np.ndarray, conf: np.ndarray = None, valid_mask: np.ndarray = None):
        &quot;&quot;&quot;
        Incrementally add a new frame of view data.

        Args:
            view (dict): a dictionary for a new view
            img (np.ndarray): rgb_img
            conf (np.ndarray, optional): 
            valid_mask (np.ndarray, optional): 
        &quot;&quot;&quot;
        try:
            new_pcd = to_numpy(view[&apos;pts3d_world&apos;]).reshape(-1, 3)
            new_rgb = to_numpy(img).reshape(-1, 3)
        except KeyError:
            print(f&quot;Warning: &apos;pts3d_world&apos; not found in the new view. Frame skipped.&quot;)
            return
        if not self.is_initialized:
            self.res_pcds = new_pcd
            self.res_rgbs = new_rgb
            if conf is not None:
                self.res_confs = to_numpy(conf).reshape(-1)
            if valid_mask is not None:
                self.res_valid_masks = to_numpy(valid_mask).reshape(-1)
            self.is_initialized = True
        else:
            self.res_pcds = np.concatenate([self.res_pcds, new_pcd], axis=0)
            self.res_rgbs = np.concatenate([self.res_rgbs, new_rgb], axis=0)
            if conf is not None:
                new_conf = to_numpy(conf).reshape(-1)
                self.res_confs = np.concatenate([self.res_confs, new_conf], axis=0)
            if valid_mask is not None:
                new_mask = to_numpy(valid_mask).reshape(-1)
                self.res_valid_masks = np.concatenate([self.res_valid_masks, new_mask], axis=0)

    def save_snapshot(self, snapshot_id: int, save_dir: str, num_points_save: int = 200000, conf_thres_res: float = 3.0):
        &quot;&quot;&quot;
        Just save
        &quot;&quot;&quot;
        if not self.is_initialized:
            print(&quot;Warning: Reconstructor not initialized. Nothing to save.&quot;)
            return
        save_name = f&quot;recon_snapshot_{snapshot_id:05d}.ply&quot;
        pts_count = len(self.res_pcds)
        final_valid_mask = np.ones(pts_count, dtype=bool)

        if self.res_valid_masks is not None:
            final_valid_mask &amp;#x26;= self.res_valid_masks
        
        if self.res_confs is not None:
            conf_masks = self.res_confs &gt; conf_thres_res
            final_valid_mask &amp;#x26;= conf_masks

        valid_ids = np.where(final_valid_mask)[0]
        
        if len(valid_ids) == 0:
            print(f&quot;Warning for snapshot {snapshot_id}: No valid points left after filtering.&quot;)
            return
            
        print(f&apos;Snapshot {snapshot_id}: Ratio of points filtered out: {(1. - len(valid_ids) / pts_count) * 100:.2f}%&apos;)
        n_samples = min(num_points_save, len(valid_ids))
        print(f&quot;Snapshot {snapshot_id}: Resampling {n_samples} points from {len(valid_ids)} valid points.&quot;)
        sampled_idx = np.random.choice(valid_ids, n_samples, replace=False)
        sampled_pts = self.res_pcds[sampled_idx]
        sampled_rgbs = self.res_rgbs[sampled_idx]
        save_path = join(save_dir, save_name)
        print(f&quot;Saving reconstruction snapshot to {save_path}&quot;)
        save_ply(points=sampled_pts, save_path=save_path, colors=sampled_rgbs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åœ¨æ¯ä¸€ä¸ªå¾ªç¯æœ€ååŠ ä»¥è°ƒç”¨ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;reconstructor.add_frame(
            view=input_views[i],
            img=rgb_imgs[i],
            conf=per_frame_res[&apos;l2w_confs&apos;][i],
            valid_mask=valid_masks
        )
        if args.save_online:
            if (i + 1) % args.save_frequency == 0:
                reconstructor.save_snapshot(
                    snapshot_id=i + 1,
                    save_dir=save_dir,
                    num_points_save=num_points_save,
                    conf_thres_res=conf_thres_l2w
                )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OKï¼Œåˆ°æ­¤ä¸ºæ­¢æˆ‘å°±å†™å®Œäº†åŸæœ¬çš„å¤„ç†é€»è¾‘çš„è§£é‡Šå’Œæ–°å†™çš„**onlinee*å¤„ç†é€»è¾‘ä»‹ç»ï¼Œå…¶å®è¦è¯´ä¸è¯´ï¼Œ&lt;strong&gt;online&lt;/strong&gt;å¤„ç†é€»è¾‘ä¹Ÿå¹¶éå¤ªè¿‡å¤æ‚ï¼Œä½†æ˜¯å¥ˆä½•æˆ‘è¿™å‡ å¤©å› ä¸ºå­¦è½¦è€½è¯¯äº†å¤ªå¤šæ—¶é—´ä¹Ÿæ²¡åšä»€ä¹ˆä¸œè¥¿ï¼ˆx&lt;/p&gt;
&lt;p&gt;åˆæ°´äº†ä¸€ç¯‡blogğŸ˜‹&lt;/p&gt;
&lt;h2&gt;æ–°çš„ä»“åº“ï¼š&lt;/h2&gt;
&lt;p&gt;import { GithubCard } from &apos;astro-pure/advanced&apos;&lt;/p&gt;</content:encoded><h:img src="/_astro/image.CHGvQ2OJ.png"/><enclosure url="/_astro/image.CHGvQ2OJ.png"/></item><item><title>SLAM3Rè¯»åæœ‰æ„Ÿ</title><link>https://hjcheng0602.github.io/blog/slam3r/slam3r</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/slam3r/slam3r</guid><description>æœ¬äººè¯»å®ŒSLAM3Råçš„ç†è§£å–µ</description><pubDate>Sun, 03 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;æœ€è¿‘å‡ å¤©è¯»å®Œäº†&lt;a href=&quot;https://github.com/PKU-VCL-3DV/SLAM3R&quot;&gt;SLAM3R&lt;/a&gt;çš„è®ºæ–‡ï¼Œè¿™æ˜¯2025å¹´CVPRçš„ä¸€ ç¯‡&lt;strong&gt;Highlight&lt;/strong&gt;è®ºæ–‡ï¼Œä¹Ÿæ˜¯æˆ‘åœ¨3Ræ–¹å‘çš„è¯»è¿‡çš„ç¬¬3ç¯‡è®ºæ–‡ã€‚&lt;/p&gt;
&lt;p&gt;è¿™ç¯‡è®ºæ–‡ä¸»è¦ä»‹ç»äº†ä¸€ä¸ªå«åš&lt;strong&gt;SLAM3R&lt;/strong&gt;çš„æ ¹æ®è§†é¢‘å³æ—¶é‡å»ºçš„ç³»ç»Ÿï¼Œæ„Ÿè§‰æ˜¯ç”±&lt;strong&gt;DUst3R&lt;/strong&gt;ä¸­è·å¾—çš„çµæ„Ÿï¼Œä¸åŒçš„æ˜¯&lt;strong&gt;DUst3R&lt;/strong&gt;æ˜¯æ ¹æ®ä¸¤å¼ å›¾ç‰‡é‡å»ºå‡ºä¸‰ç»´ç‚¹å›¾ï¼Œå¹¶ä¸”æ˜¯ç¦»çº¿å¤„ç†ï¼›è€Œ&lt;strong&gt;SLAM3R&lt;/strong&gt;æ˜¯ä»ä¸€ä¸ªå•ç›®è§†é¢‘ä¸­å®æ—¶åœ¨çº¿é‡å»ºï¼Œå¹¶ä¸”ç›¸è¾ƒäºä¹‹å‰çš„ä¸€äº›æ–¹æ³•å…·æœ‰æé«˜çš„æ•ˆç‡ã€‚&lt;/p&gt;
&lt;h2&gt;SLAM3Rçš„ä¸»è¦æ¨¡å—&lt;/h2&gt;
&lt;p&gt;SLAM3Rä¸»è¦ç”±&lt;strong&gt;I2P&lt;/strong&gt;å’Œ&lt;strong&gt;L2W&lt;/strong&gt;ä¸¤å¤§æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£ä»è§†é¢‘ä¸­çš„å…³é”®å¸§é‡å»ºç‚¹å›¾(Image to Point)å’Œåˆ©ç”¨ç‚¹å›¾å¢é‡å¼åœ°é‡å»ºå…¨å±€ç‚¹å›¾ï¼ˆLocal to Worldï¼‰,å…·ä½“ç»“æ„å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./overalmodule.png&quot; alt=&quot;nothing&quot;&gt;&lt;/p&gt;
&lt;h3&gt;è§†é¢‘é¢„å¤„ç†&lt;/h3&gt;
&lt;p&gt;é¦–å…ˆï¼ŒSLAM3Ré‡‡ç”¨äº†æ»‘åŠ¨çª—å£ç®—æ³•å°†è§†é¢‘æ‹†æˆå¤šä¸ªå°ç‰‡æ®µï¼ŒæŠŠå¤šä¸ªå°ç‰‡æ®µè¾“å…¥åˆ°I2Pä¸­è¿›è¡Œå¤„ç†ã€‚&lt;/p&gt;
&lt;h3&gt;I2Pç½‘ç»œ&lt;/h3&gt;
&lt;p&gt;I2Pæ¨¡å—æ¥å—é¢„å¤„ç†äº§ç”Ÿçš„è§†é¢‘ç‰‡æ®µï¼Œè¯¥è§†é¢‘ç‰‡æ®µç”±å¤šä¸ªå¸§${F_i},i = 1, ... N$ç»„æˆã€‚é€šå¸¸æˆ‘ä»¬ä»ä¸­é€‰å–æœ€ä¸­é—´çš„å¸§ä½œä¸ºå…³é”®å¸§$F_{key}$ï¼Œå‰©ä¸‹çš„$N - 1$ä¸ªå¸§ä½œä¸ºè¡¥å……å¸§è¾“å…¥åˆ°I2Pä¸­ã€‚&lt;/p&gt;
&lt;p&gt;é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å¸§é€šè¿‡ä¸€ä¸ªç”±$m$ä¸ªViT encoderç»„æˆçš„$E_{img}$ï¼Œç”Ÿæˆç›¸åº”çš„tokenï¼Œç„¶åå†è¿›è¡Œdecoderæ“ä½œã€‚å…·ä½“å°±æ˜¯å°†å…³é”®å¸§çš„tokenè¾“å…¥åˆ°ä¸€ä¸ªç‰¹æ®Šå¤„ç†çš„decoder:$D_{key}$é‡Œï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œç„¶åå‰©ä¸‹çš„$N - 1$ä¸ªè¡¥å……å¸§å…±äº«åŒä¸€ä¸ªdecoderç»“æ„ï¼ˆç»§æ‰¿è‡ª&lt;strong&gt;DUst3R&lt;/strong&gt;ï¼Œç”±$n$ä¸ªViT decoderç»„æˆï¼‰ï¼Œå‡ç”Ÿæˆå¯¹åº”çš„$G_{sup_i}$ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./D_key.png&quot; alt=&quot;$D_key$&quot;&gt;&lt;/p&gt;
&lt;p&gt;ç„¶åï¼Œæˆ‘ä»¬å†ä½¿ç”¨ç±»ä¼¼äº&lt;strong&gt;DUSt3R&lt;/strong&gt;ä¸­çš„æ–¹æ³•ï¼Œå°†è¿™äº›å¸§ï¼ˆå°¤å…¶æ˜¯å…³é”®å¸§ï¼‰åšå‡ºä¸€ä¸ªç½®ä¿¡åº¦æœ€é«˜çš„ä¸‰ç»´é‡å»ºã€‚ä»è€Œå¾—åˆ°æŸä¸€ä¸ªè§†é¢‘ç‰‡æ®µå¯¹åº”çš„ç‚¹å›¾$\hat{X}_{key}$ã€‚&lt;/p&gt;
&lt;h3&gt;L2Wç½‘ç»œ&lt;/h3&gt;
&lt;p&gt;è¿™ä¸ªæ¨¡å—æ¥å—I2Pæ¨¡å—äº§ç”Ÿçš„$X_{key}$ä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºå…¶æ˜¯ä¸€ä¸ªåœ¨çº¿å¤„ç†æ–¹æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥äº†ç¼“å†²é›†è¿™ä¸€å…³é”®çš„ç»„åˆ†ã€‚&lt;/p&gt;
&lt;p&gt;é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨å·²ç»å¤„ç†å®Œçš„å…³é”®å¸§ç‚¹å›¾ä¸­é‡‡ç”¨&lt;code&gt;reservoir strategy&lt;/code&gt;é€‰å–$B$ä¸ªå·²ç»æ³¨å†Œå®Œçš„å¸§ä½œä¸ºç¼“å†²é›†ï¼ˆå¯¹äºç¬¬ä¸€ä¸ªå¸§è¿™ç§ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é‡å¤è¿è¡Œå¤šæ¬¡I2Pè·å–è¶³å¤Ÿå¤šæ•°é‡çš„åˆå§‹å¸§ä½œä¸ºç¼“å†²é›†ï¼‰ï¼Œç„¶åï¼Œæ¯å½“ä¸€ä¸ªæ–°çš„å¸§è¾“å…¥æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼ˆç”±I2Pä¸­çš„decoderç»„æˆï¼‰åœ¨ç¼“å†²é›†ä¸­å°†ç‰¹å¾çš„ç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…ï¼Œæˆ‘ä»¬ç„¶åé€‰å–åŒ¹é…åº¦æœ€é«˜çš„$K$ä¸ªå…³é”®å¸§ç‚¹å›¾ï¼Œç„¶åå°†è¿™$K$ä¸ªå…³é”®å¸§ç‚¹å›¾ $\hat{X}_{i}^{H \times W \times 3},i = 1 , ..., K + 1$ä½œä¸ºè¿™ä¸ªæ¨¡å—çš„è¾“å…¥ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚å‰å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†è¿™$K + 1$ä¸ªç‚¹å›¾è¾“å…¥åˆ°æˆ‘ä»¬çš„L2Wæ¨¡å—çš„encoder $E_{pts}$ ä¸­ï¼š
$$
\mathcal{P}&lt;em&gt;i^{(T\times d)}=E&lt;/em&gt;{pts}(\hat{X}_i^{(H\times W\times3)}),i=1,...,K+1.
$$
ç„¶åï¼Œç”±äºæˆ‘ä»¬å®é™…ä¸Šä¸èƒ½åªé€šè¿‡ç‚¹å›¾ä¿¡æ¯æ¥è¿›è¡Œå»ºæ¨¡ï¼ˆå¦‚çº¹ç†ç›¸åŒçš„ä¸¤ä¸ªä¸ä¸€æ ·çš„å¹³é¢æˆ–ä¸åŒçš„ä¸€å—åœ°é¢ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©å°†ç‰¹å¾ä¸I2Pç½‘ç»œä¸­çš„ç‰¹å¾èåˆï¼š
$$
\mathcal{F}_i^{(T\times d)}=F_i^{(T\times d)}+\mathcal{P}_i^{(T\times d)},i=1,...,K+1.
$$
åœ¨è¿™ä¹‹åï¼Œæˆ‘ä»¬ä¾¿ç”Ÿæˆäº†æ¯å¼ ç‚¹å›¾çš„ä½ç½®å¤–è§‚ç‰¹å¾åºåˆ—ã€‚&lt;/p&gt;
&lt;p&gt;ç´§æ¥ç€ï¼Œæˆ‘ä»¬ä¼šè¿™$K + 1$ä¸ªç‚¹å›¾è¾“å…¥åˆ°ä¸¤ä¸ªè§£ç å™¨ä¸­ï¼š&lt;/p&gt;
&lt;h4&gt;Registration Decoder&lt;/h4&gt;
&lt;p&gt;Registration Decoderå°†æ‰€æœ‰tokenä½œä¸ºè¾“å…¥ï¼Œç„¶åç›®çš„æ˜¯å°†L2Wçš„å…³é”®å¸§é‡å»ºè½¬æ¢åˆ°åœºæ™¯åæ ‡ç³»ä¸‹ï¼Œå®ƒä¸$D_{key}$é‡‡ç”¨ç›¸åŒçš„æ¶æ„ã€‚&lt;/p&gt;
&lt;p&gt;è§£ç è¿‡ç¨‹å¤§æ¦‚æ˜¯ï¼š
$$
\mathcal{G}&lt;em&gt;{sce_i}=D&lt;/em&gt;{sce}(\mathcal{F}&lt;em&gt;{sce_i},\mathcal{F}&lt;/em&gt;{key}),\quad i=1,...,K
$$&lt;/p&gt;
&lt;h4&gt;Scene Decoder&lt;/h4&gt;
&lt;p&gt;Scene DecoderåŒæ ·å°†æ‰€æœ‰tokenä½œä¸ºè¾“å…¥ï¼Œä½†æ˜¯å®ƒçš„ç›®çš„æ˜¯åœ¨ä¸æ”¹å˜åœºæ™¯åæ ‡ç³»çš„æƒ…å†µä¸‹ï¼Œç²¾åŒ–åæ ‡å‡ ä½•ã€‚ä»–åŒæ ·é‡‡ç”¨ä¸$D_{key}$ç›¸åŒçš„æ¶æ„ï¼Œä½†æ˜¯ä»–æ˜¯å¯¹æ¯ä¸€ä¸ªåœ¨å·²é€‰ä¸­çš„å…³é”®å¸§ç‚¹å›¾è¿›è¡Œä¼˜åŒ–ï¼š
$$
\mathcal{G}&lt;em&gt;{sce_i}=D&lt;/em&gt;{sce}(\mathcal{F}&lt;em&gt;{sce_i},\mathcal{F}&lt;/em&gt;{key}),\quad i=1,...,K
$$
é€šè¿‡è¿™æ ·çš„æ–¹å¼å°†å·²ç”Ÿæˆçš„point mapè¿›è¡Œä¼˜åŒ–&lt;/p&gt;
&lt;p&gt;æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨ç±»ä¼¼äºI2Pæ¨¡å—ä¸­çš„æ–¹æ³•å¯¹æˆ‘ä»¬æ‰€æœ‰å·²ç»é‡å»ºçš„å…³é”®å¸§tokenè¿›è¡Œç‚¹å›¾é‡å»ºï¼š
$$
\tilde{X}_i^{(H\times W\times3)},\tilde{C}_i^{(H\times W\times1)}=\mathrm{H}(\mathcal{G}_i^{(T\times d)}),i=1,...,K+1.
$$&lt;/p&gt;
&lt;p&gt;å¾—åˆ°ä¸€ä¸ªå®æ—¶çš„ä¸‰ç»´è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;h2&gt;ç»“è®º&lt;/h2&gt;
&lt;p&gt;æœ¬äººç›®å‰æ¶‰çŒä¸æ·±ï¼Œä½†æ˜¯è®ºæ–‡æœ€åä¸å…¶ä»–ç³»ç»Ÿåšæ¯”è¾ƒï¼Œå…¶å±•ç°çš„æ•ˆç‡ç¡®å®ä»¤æˆ‘å°è±¡æ·±åˆ»ï¼Œæ„Ÿè§‰ä»¥ä¸Šçš„è¿™ä¸ªç³»ç»Ÿçš„ä¸¤å¤§æ¨¡å—ä¹Ÿä»¤éå¸¸ç®€æ´èˆ’é€‚ã€‚ç­‰æˆ‘å†å»é˜…è¯»å…¶ä»–çš„3Ræ–‡ç« æ¥è¿›ä¸€æ­¥ç†è§£è¿™ä¸ªSOTAçš„å«é‡‘é‡å§ğŸ˜‹&lt;/p&gt;
&lt;p&gt;githubé¡¹ç›®åœ°å€ï¼š&lt;/p&gt;
&lt;p&gt;import { GithubCard } from &apos;astro-pure/advanced&apos;&lt;/p&gt;
&lt;p&gt;å–µå–µåˆæ˜¯å……å®çš„ä¸€å¤©ğŸ¥³ï¼Œæœ¬äººå¯èƒ½ç†è§£æœ‰åå·®ï¼ˆbushi&lt;/p&gt;</content:encoded><h:img src="/_astro/cover.uwodCb2H.png"/><enclosure url="/_astro/cover.uwodCb2H.png"/></item><item><title>Celebrate and Introduce My First Page</title><link>https://hjcheng0602.github.io/blog/celebrate</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/celebrate</guid><description>Just celebrate this page as a milestone and introduce the future of the site</description><pubDate>Sat, 02 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Here, I build my first &lt;a href=&quot;https://hjcheng0602.github.io/&quot;&gt;website&lt;/a&gt;(not the first but the first one I&apos;m serious about building/running)ğŸ˜‹.&lt;/p&gt;
&lt;p&gt;My website will include:&lt;/p&gt;
&lt;h2&gt;study course expriences&lt;/h2&gt;
&lt;p&gt;This kind of content will record my experiences learning some meaningful courses in PKU.I hope it will help me review my courses.&lt;/p&gt;
&lt;h2&gt;research experiences&lt;/h2&gt;
&lt;p&gt;As a college student, researching and finding will be the main task in the future. Currently I am interested in 3R(3D reconstruction). So maybe I will update huge contents about my reflections for each paper.&lt;/p&gt;
&lt;h2&gt;my own projects&lt;/h2&gt;
&lt;p&gt;Of course, my some great(just in my standard) project will be post on the site. It&apos;s meaningful to me as long as I think it&apos;s great, regardless of how others see it.&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;Above might be the main topics of content in the site.&lt;/p&gt;
&lt;h3&gt;Additions&lt;/h3&gt;
&lt;p&gt;The posts will be in Chinese and English randomly(maybe most time ChineseğŸ¤£).Please forgive my poor English.&lt;/p&gt;</content:encoded><h:img src="undefined"/><enclosure url="undefined"/></item><item><title>Mast3Réƒ½ä¹‹åçš„å‘ç°</title><link>https://hjcheng0602.github.io/blog/mst3r/mast3r</link><guid isPermaLink="true">https://hjcheng0602.github.io/blog/mst3r/mast3r</guid><description>Markdown æ˜¯ä¸€ç§è½»é‡çº§çš„ã€Œæ ‡è®°è¯­è¨€ã€ã€‚</description><pubDate>Wed, 26 Jul 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;111&lt;/p&gt;</content:encoded><h:img src="undefined"/><enclosure url="undefined"/></item></channel></rss>