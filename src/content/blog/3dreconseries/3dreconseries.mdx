---
title: 3D Reconstruction Series
publishDate: 4096-16-64 8:18:00
description: '一个长期更新的3D recon(或许还有generate)论文的浅要阅读'
tags:
- 3Dreconstruction
- paper reading
language: '中文'
draft: false
heroImage: { src: './cover.png', color: '#B4C6DA' }
---

## 引言
可以看到本文的publishDate是4096-16-64, 实际上的publishDate是2026-02-10。
本文的初衷是一个长期更新的3D recon系列论文阅读，之前其实已经发过了一些该领域的论文的精读了，但是显然精读必然是不可长期持续的。因此，我想以本文——一个系列的形式记录对大多数论文的浅要阅读，当然如果有特别重要的论文，我也会单开一篇文章进行精读的。

本文的cover image是一个词云，记录了本文包含的工作的名称，希望它能不断地更新，成为一个3D recon领域的词云图谱。

import { GithubCard } from 'astro-pure/advanced'

## CUT3R

CUT3R的输入是视频序列，但是也可以unordered（据作者所言训练的时候是无序训练的，但是推理的时候推理的时候是dataloader先计算重合率来进行初步排序。），使用一个feed forward网络预测camera parameters和点云。

![cut3r](./CUT3R.png)

然后是一个recurrent模型，每一帧输入的时候添加一个pose token然后经过encoder和decoder，之后使用交叉注意力更新$s_t$ 和 $F_t$，之后再使用不同head来从$s_t$和$F_t$中提取output。

显然这样缺少修正，对于长序列容易造成偏移。但是作者似乎也提到了一个revisit机制，在输入结束之后拿着全局的$s$来做之前的预测，在7scene上的acc和comp是有改善的，但是NRGBD不怎么明显。

此外，作者也说因为数据集质量的原因，采用的head即使已经有一个pose head和local points head，也仍然要加入一个world ptshead（缺乏高质量的数据集）。
<GithubCard repo='CUT3R/CUT3R' />
## $\pi ^ 3$



$\pi ^ 3$ 是一个相对来说比较有趣的东西，模型结构如下：

![pi3](./Pi3.png)

首先与之前的最大不同是它没有显式地选取参考帧和一个特定的scale factor，像VGGT就是先选取了一个ref frame然后做重建，但是重建质量受ref影响很大，因此$\pi^3$选择了一个方案，就是一次性将所有帧全部输入，所有帧之间均平等，然后inference出一组相对位姿和局部点云，这样就能规避确定某一个frame作为坐标原点造成的不确定性问题。

但是仔细一想，$\pi^3$仍然不怎么好避免一个ref的问题，首先，在一个batch内部，虽然我们预测的是一组相对位姿，但是直觉上感觉仍然是把**某一帧与其他帧不融洽**所导致的原先的那种**大的，显著的，偶然性的**损失转化为了现在的**看起来不明显的、高一致的、所有帧都有的**系统性损失。但作者通过实验证明了损失会变小，其实这也是比较好解释的，因为原先的可能是$T_2$依赖$T_1$，$T_3$依赖$T_2$……这种**单向参考**，而$\pi^3$则进行了**交叉注意力计算**，仔细想来确实会更好。

其次，交叉注意力的复杂度大概是$O(n^2)$，显然对于长序列是不可接受的，作者训练和测试的时候均采用了有限个batch内frame的做法，但对于实际的长序列的话，感觉并不是很好做。如果切片进行拼接的话，显然也会面临ref的选择问题，但是这时候是一个scene之间的拼接，感觉确实会降低很多错误，如果分层做的话，也会降低误差，总之感觉似乎确实是一个不错的方案。
<GithubCard repo='yyfz/Pi3' />
## DA3

DA3是字节seed的一个项目，可以说是力大飞砖，充分体现了工业界解决问题的规模（x。





![da3](./DA3.png)
DA3的主要创新点在于：

- **更简单的模型**，作者的意思是VGGT即使结构很简单，但是由于其在DINO后接AA层的操作，因为AA layers是新训练的，因此过程中可能数据的利用率不高。而DA3选择了只利用DINO这一个方案，通过在DINO的$L_g$层中变形数据完成了AA层所做的事情。因此，DA3的几乎所有参数都是预训练过的，而vggt则有$\frac{2}{3}$ 的参数是从头开始训的，这是DA3的简洁之处。

- **预测任务的简洁性**。相比于VGGT通过不同head得出了不同结果，DA3则使用了一个更新的表达方式：Ray-depth表达，具体来说就是使用一个Dual head来分别输出一个像素的深度信息和光心与之相连的射线的信息，从而天然地同时包含了点云和pose信息，而且在设计loss的时候是可以加入一致性信息的。相比与vggt，这似乎加强了一致性，也提高了数据利用率，感觉pose和pts3d反而是不容易加入一致性的，作者做的消融实验也证实了这一点。

- **使用teacher标定数据**，首先训了一个teacher模型用于给深度不好的frame重新生成depth，之后依照这个depths训练。感觉最终效果也很依赖这个teacher模型。

但是，DA3的弊端也有一些，他的效果确实非常好，但是阅读之后才发现他是用128 x H100训练的，这个规模确实有点难以复现。小算力情况下上面两条结论似乎很有帮助，可以尝试。
<GithubCard repo='ByteDance-Seed/depth-anything-3' />