---
title: 3D Reconstruction Series
publishDate: 4096-16-64 8:18:00
description: '一个长期更新的3D recon(或许还有generate)论文的浅要阅读'
tags:
- 3Dreconstruction
- paper reading
language: '中文'
draft: false
heroImage: { src: './cover.png', color: '#B4C6DA' }
---

## 引言
可以看到本文的publishDate是4096-16-64, 实际上的publishDate是2026-02-10。
本文的初衷是一个长期更新的3D recon系列论文阅读，之前其实已经发过了一些该领域的论文的精读了，但是显然精读必然是不可长期持续的。因此，我想以本文——一个系列的形式记录对大多数论文的浅要阅读，当然如果有特别重要的论文，我也会单开一篇文章进行精读的。

本文的cover image是一个词云，记录了本文包含的工作的名称，希望它能不断地更新，成为一个3D recon领域的词云图谱。

import { GithubCard } from 'astro-pure/advanced'

## CUT3R
<GithubCard repo='CUT3R/CUT3R' />
CUT3R的输入是视频序列，但是也可以unordered（据作者所言训练的时候是无序训练的，但是推理的时候推理的时候是dataloader先计算重合率来进行初步排序。），使用一个feed forward网络预测camera parameters和点云。

![cut3r](./CUT3R.png)

然后是一个recurrent模型，每一帧输入的时候添加一个pose token然后经过encoder和decoder，之后使用交叉注意力更新$s_t$ 和 $F_t$，之后再使用不同head来从$s_t$和$F_t$中提取output。

显然这样缺少修正，对于长序列容易造成偏移。但是作者似乎也提到了一个revisit机制，在输入结束之后拿着全局的$s$来做之前的预测，在7scene上的acc和comp是有改善的，但是NRGBD不怎么明显。

此外，作者也说因为数据集质量的原因，采用的head即使已经有一个pose head和local points head，也仍然要加入一个world ptshead（缺乏高质量的数据集）。

## $\pi ^ 3$

<GithubCard repo='yyfz/Pi3' />

$\pi ^ 3$ 是一个相对来说比较有趣的东西，模型结构如下：

![pi3](./Pi3.png)

首先与之前的最大不同是它没有显式地选取参考帧和一个特定的scale factor，像VGGT就是先选取了一个ref frame然后做重建，但是重建质量受ref影响很大，因此$\pi^3$选择了一个方案，就是一次性将所有帧全部输入，所有帧之间均平等，然后inference出一组相对位姿和局部点云，这样就能规避确定某一个frame作为坐标原点造成的不确定性问题。

但是仔细一想，$\pi^3$仍然不怎么好避免一个ref的问题，首先，在一个batch内部，虽然我们预测的是一组相对位姿，但是直觉上感觉仍然是把**某一帧与其他帧不融洽**所导致的原先的那种**大的，显著的，偶然性的**损失转化为了现在的**看起来不明显的、高一致的、所有帧都有的**系统性损失。但作者通过实验证明了损失会变小，其实这也是比较好解释的，因为原先的可能是$T_2$依赖$T_1$，$T_3$依赖$T_2$……这种**单向参考**，而$\pi^3$则进行了**交叉注意力计算**，仔细想来确实会更好。

其次，交叉注意力的复杂度大概是$O(n^2)$，显然对于长序列是不可接受的，作者训练和测试的时候均采用了有限个batch内frame的做法，但对于实际的长序列的话，感觉并不是很好做。如果切片进行拼接的话，显然也会面临ref的选择问题，但是这时候是一个scene之间的拼接，感觉确实会降低很多错误，如果分层做的话，也会降低误差，总之感觉似乎确实是一个不错的方案。